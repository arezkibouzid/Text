{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv('./data/Tweets.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                                                  @VirginAmerica What @dhepburn said.   \n",
       "1                             @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                              @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &...   \n",
       "4                                              @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df = load_data()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (14640, 15)\n",
      "Columns are: Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
      "       'negativereason', 'negativereason_confidence', 'airline',\n",
      "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
      "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
      "       'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      "tweet_id                        14640 non-null int64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null object\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size:',tweet_df.shape)\n",
    "print('Columns are:',tweet_df.columns)\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : take \"text\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                    @VirginAmerica What @dhepburn said.\n",
       "1                               @VirginAmerica plus you've added commercials to the experience... tacky.\n",
       "2                                @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
       "3    @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &...\n",
       "4                                                @VirginAmerica and it's a really big bad thing about it\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_minus = tweet_df['text']\n",
    "tweet_df_minus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We will perform the following steps: \n",
    "\n",
    "1. **Tokenization** : Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "2. Words that have fewer than 3 characters are removed.\n",
    "3. All **stopwords** are removed.\n",
    "4. Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "5. Words are **stemmed** - words are reduced to their root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lbenboudiaf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words_deleted_length = 3\n",
    "Lemmatized = True\n",
    "Stemmed = False\n",
    "\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    if(Lemmatized & Stemmed):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    if(Stemmed):\n",
    "        return stemmer.stem(text)\n",
    "    if(Lemmatized):\n",
    "        return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "    return text\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > Words_deleted_length:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of not pre-processeded data :  14640\n",
      "Size of pre-processeded data :  14640\n",
      "\n",
      "Check data processing effect:\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']\n"
     ]
    }
   ],
   "source": [
    "'''We create two lists, one for pre-processed data and an other for not pre-processed data'''\n",
    "processed_docs = []\n",
    "nan_processed_docs = []\n",
    "for doc in tweet_df_minus :\n",
    "    nan_processed_docs.append(doc)\n",
    "for doc in tweet_df_minus :\n",
    "    processed_docs.append(preprocess(doc))\n",
    "    \n",
    "\n",
    "'''They should have the same size''' \n",
    "print('Size of not pre-processeded data : ', len(nan_processed_docs))\n",
    "print('Size of pre-processeded data : ', len(processed_docs))\n",
    "print('\\nCheck data processing effect:')\n",
    "print(nan_processed_docs[1])\n",
    "print(processed_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Bag of words on the dataset\n",
    "\n",
    "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass processed_docs to gensim.corpora.Dictionary() and call it 'dictionary'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(9775 unique tokens: ['amypoehler', 'understand', 'friday', 'average', 'thanksm']...)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of appearence: 4188 Word: amypoehler\n",
      "Number of appearence: 498 Word: understand\n",
      "Number of appearence: 509 Word: friday\n",
      "Number of appearence: 2993 Word: average\n",
      "Number of appearence: 9071 Word: thanksm\n",
      "Number of appearence: 7324 Word: attendees\n",
      "Number of appearence: 6072 Word: spill\n",
      "Number of appearence: 7169 Word: livery\n",
      "Number of appearence: 4677 Word: bijlxat\n",
      "Number of appearence: 9508 Word: banana\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 1\n",
    "for k, v in dictionary.iteritems():\n",
    "    print('Number of appearence:', k ,'Word:',v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The representation is a tuple : (word_id, word_frequency) for example : \n",
      "\n",
      "[['virginamerica', 'dhepburn', 'say'], ['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)], [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e. for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print('The representation is a tuple : (word_id, word_frequency) for example : \\n')\n",
    "print(processed_docs[:2])\n",
    "bow_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"virginamerica\") appears 1 time.\n",
      "Word 24 (\"seat\") appears 1 time.\n",
      "Word 28 (\"time\") appears 1 time.\n",
      "Word 88 (\"available\") appears 1 time.\n",
      "Word 89 (\"carriers\") appears 1 time.\n",
      "Word 90 (\"fare\") appears 1 time.\n",
      "Word 91 (\"select\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running LDA using Bag of Words\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "We will be running LDA using all CPU cores to parallelize and speed up model training.\n",
    "\n",
    "Some of the parameters we will be tweaking are:\n",
    "\n",
    "1. **num_topics** is the number of requested latent topics to be extracted from the training corpus.\n",
    "2. **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "3. **workers** is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "4. **alpha** and **beta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is 1/num_topics)\n",
    "\n",
    " **Alpha** is the per document topic distribution.\n",
    "   - *High alpha* : Every document has a mixture of all topics(documents appear similar to each other).\n",
    "   - *Low alpha* : Every document has a mixture of very few topics\n",
    "\n",
    "  **bEta** is the per topic word distribution.\n",
    "    - *High beta* : Each topic has a mixture of most words(topics appear similar to each other).\n",
    "    - *Low eta* : Each topic has a mixture of few words.\n",
    "\n",
    "5. **passes** is the number of training passes through the corpus. For example, if the training corpus has 50,000 documents, chunksize is 10,000, passes is 2, then online training is done in 10 updates:\n",
    "- documents 0-9,999\n",
    "- documents 10,000-19,999\n",
    "- documents 20,000-29,999\n",
    "- documents 30,000-39,999\n",
    "- documents 40,000-49,999\n",
    "- documents 0-9,999\n",
    "- documents 10,000-19,999\n",
    "- documents 20,000-29,999\n",
    "- documents 30,000-39,999\n",
    "- documents 40,000-49,999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(corpus=bow_corpus, \n",
    "                                   num_topics = 8,\n",
    "                                   chunksize=100,\n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"customers\" + 0.021*\"plan\" + 0.020*\"tonight\" + 0.020*\"appreciate\" + 0.019*\"airlines\" + 0.019*\"passengers\" + 0.019*\"sure\" + 0.017*\"gate\" + 0.015*\"money\" + 0.015*\"suppose\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.137*\"flight\" + 0.054*\"cancel\" + 0.030*\"hours\" + 0.030*\"wait\" + 0.029*\"hold\" + 0.029*\"jetblue\" + 0.027*\"delay\" + 0.026*\"flightled\" + 0.019*\"plane\" + 0.017*\"time\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.079*\"book\" + 0.056*\"ticket\" + 0.038*\"rebooked\" + 0.027*\"problems\" + 0.025*\"follow\" + 0.023*\"miles\" + 0.018*\"understand\" + 0.016*\"credit\" + 0.016*\"ask\" + 0.016*\"receive\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.047*\"change\" + 0.037*\"try\" + 0.036*\"online\" + 0.028*\"reservation\" + 0.027*\"check\" + 0.021*\"mins\" + 0.018*\"reservations\" + 0.013*\"charge\" + 0.013*\"stop\" + 0.013*\"tuesday\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.253*\"americanair\" + 0.083*\"usairways\" + 0.033*\"flight\" + 0.033*\"thank\" + 0.025*\"service\" + 0.022*\"help\" + 0.020*\"customer\" + 0.013*\"need\" + 0.012*\"http\" + 0.011*\"guy\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.051*\"luggage\" + 0.041*\"lose\" + 0.032*\"agents\" + 0.031*\"people\" + 0.030*\"speak\" + 0.027*\"baggage\" + 0.021*\"nice\" + 0.018*\"wasn\" + 0.018*\"extra\" + 0.016*\"tweet\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.074*\"seat\" + 0.031*\"refund\" + 0.025*\"say\" + 0.023*\"http\" + 0.022*\"issue\" + 0.020*\"busy\" + 0.020*\"class\" + 0.019*\"pay\" + 0.018*\"break\" + 0.018*\"upgrade\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.041*\"number\" + 0.038*\"airline\" + 0.027*\"worst\" + 0.017*\"contact\" + 0.015*\"connection\" + 0.013*\"offer\" + 0.013*\"hotel\" + 0.012*\"seriously\" + 0.012*\"allow\" + 0.012*\"better\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3425000748081727\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the baseline coherence score for the default LDA model, let’s perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
    "\n",
    "    Number of Topics (K)\n",
    "    Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "    Dirichlet hyperparameter beta: Word-Topic Density\n",
    "We’ll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two different validation corpus sets. We’ll use **C_v** as our choice of metric for performance comparison\n",
    "\n",
    "We create function ***compute_coherence_values*** and iterate it over the range of topics, alpha, and beta parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=10, \n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 23/540 [13:25<5:51:18, 40.77s/it]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "grid = {} \n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(bow_corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(bow_corpus, num_of_docs*0.75), \n",
    "               bow_corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=8, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: create T_e_i an input to have $t_{ei}$ after apply Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['americanair',\n",
       "  'http',\n",
       "  'jetblue',\n",
       "  'need',\n",
       "  'usairways',\n",
       "  'work',\n",
       "  'check',\n",
       "  'southwestair',\n",
       "  'people',\n",
       "  'good'],\n",
       " ['jetblue',\n",
       "  'flight',\n",
       "  'unite',\n",
       "  'delay',\n",
       "  'usairways',\n",
       "  'southwestair',\n",
       "  'plane',\n",
       "  'thank',\n",
       "  'americanair',\n",
       "  'http'],\n",
       " ['americanair',\n",
       "  'usairways',\n",
       "  'service',\n",
       "  'thank',\n",
       "  'customer',\n",
       "  'unite',\n",
       "  'southwestair',\n",
       "  'wait',\n",
       "  'time',\n",
       "  'luggage'],\n",
       " ['flight',\n",
       "  'americanair',\n",
       "  'cancel',\n",
       "  'usairways',\n",
       "  'southwestair',\n",
       "  'help',\n",
       "  'flightled',\n",
       "  'hold',\n",
       "  'book',\n",
       "  'try']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_topics=lda_model.show_topics(formatted=False)\n",
    "list_topics \n",
    "\n",
    "topic = []\n",
    "T_e_i = []\n",
    "for tup in list_topics:\n",
    "    topic = []\n",
    "    for tup2 in tup[1]:\n",
    "        topic.append(tup2[0])\n",
    "    T_e_i.append(topic) \n",
    "    \n",
    "T_e_i  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Train Doc2Vec on non-pre-processed data \"nan_processed_docs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "def read(fname):\n",
    "    for i, line in enumerate(f):\n",
    "        tokens = fname[i]\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_processed_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(nan_processed_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_nan_processed_docs)\n",
    "model.train(train_nan_processed_docs, total_examples=len(nan_processed_docs), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Apply Doc2Vec on each document in your data to form $d_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['virginamerica', 'dhepburn', 'say'], ['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']]\n",
      "[array([-8.1121130e-03, -8.2422188e-03, -8.5009653e-03,  9.4700670e-03,\n",
      "       -6.1424044e-03, -6.0291924e-03,  9.3967384e-03, -3.2938537e-03,\n",
      "       -7.4124401e-03,  4.4365735e-03,  3.3982443e-03,  5.7742302e-03,\n",
      "       -2.0059585e-03, -6.6704517e-03,  3.3299492e-03,  8.7660858e-03,\n",
      "        3.0309090e-03,  9.5953336e-03,  2.2019406e-03, -3.9082011e-03,\n",
      "       -5.5418705e-04, -7.8981556e-03,  1.1231005e-03, -7.9345554e-03,\n",
      "        2.9577192e-05,  9.9902209e-03,  5.6541548e-03, -1.1336298e-03,\n",
      "       -9.5684947e-03,  7.5632879e-03,  3.8973801e-03,  1.7543974e-04,\n",
      "        3.7038026e-03,  6.9300961e-03, -5.4608597e-03,  5.8965799e-03,\n",
      "       -8.2795974e-03, -2.2272721e-03,  1.6476009e-03,  4.3660291e-03,\n",
      "        3.5117818e-03,  3.2109192e-03, -2.7814955e-03, -6.1880853e-03,\n",
      "        8.8355839e-03, -2.6708855e-03,  3.3353553e-03,  7.7089434e-03,\n",
      "        3.2863969e-03, -6.2256311e-03], dtype=float32), array([-0.00936932, -0.00698564, -0.00864459, -0.00804482, -0.00636543,\n",
      "       -0.00064517, -0.00104701, -0.00397581,  0.00162256,  0.00684583,\n",
      "        0.00209305, -0.00765901, -0.00698227,  0.00916744, -0.00988943,\n",
      "        0.00389979,  0.00556994,  0.00624627, -0.00465707,  0.00135264,\n",
      "        0.0082452 , -0.00157203, -0.00198241,  0.00060964,  0.00593629,\n",
      "        0.00490234, -0.00305704,  0.0047025 ,  0.00374744,  0.00643385,\n",
      "        0.00238679,  0.00585797, -0.00134428, -0.00551607,  0.00303706,\n",
      "        0.00999977,  0.00207013, -0.00232197, -0.00882894, -0.00261495,\n",
      "        0.00425575, -0.00238884,  0.00022205, -0.00496993,  0.00836674,\n",
      "        0.00710091, -0.00202338, -0.006395  ,  0.00733856,  0.00524756],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "Liste_D_e_i = []\n",
    "Liste_of_n_docs = processed_docs\n",
    "for i in range(len(Liste_of_n_docs)):\n",
    "    vector = []\n",
    "    vector = model.infer_vector(Liste_of_n_docs[i])\n",
    "    Liste_D_e_i.append(vector) \n",
    "    \n",
    "print(Liste_of_n_docs[:2])\n",
    "print(Liste_D_e_i[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Apply Doc2Vec on your topics to form $t_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.6189611e-03, -5.7761609e-03,  8.8887429e-03,  7.1812181e-05,\n",
       "         1.3372501e-03,  2.2232926e-03,  3.3773489e-03, -5.0686724e-03,\n",
       "         2.3342411e-03,  9.5074857e-03, -5.6510912e-03,  4.9197194e-03,\n",
       "        -6.3650357e-03,  8.8025462e-03, -4.6121841e-03, -4.7187209e-03,\n",
       "        -6.0499255e-03,  8.9482125e-03,  4.5692851e-03, -1.5405561e-04,\n",
       "        -4.2776135e-03,  7.6732994e-03, -9.7185532e-03, -1.5098313e-03,\n",
       "         2.4043793e-04,  3.5099888e-03,  6.6660470e-03, -6.9918861e-03,\n",
       "         7.2210683e-03, -9.8983021e-03,  1.9597057e-03, -1.4058575e-03,\n",
       "         4.4692433e-03, -7.0016026e-03,  4.2176726e-03, -7.0681684e-03,\n",
       "         9.9102091e-03, -7.9297964e-03,  6.6172373e-03,  9.6123023e-03,\n",
       "         5.0199782e-03,  8.7301964e-03, -4.1522384e-03,  4.8934389e-03,\n",
       "         2.2044850e-03,  9.0630125e-04,  3.2948253e-03, -8.0621773e-03,\n",
       "         4.5056785e-03,  5.5571552e-03], dtype=float32),\n",
       " array([ 0.00275685, -0.00764542, -0.009705  ,  0.00741494, -0.00103935,\n",
       "         0.00027869, -0.0067296 ,  0.00251352,  0.0074662 ,  0.0029708 ,\n",
       "        -0.00459028, -0.00665765,  0.00294651,  0.0040573 , -0.00127593,\n",
       "         0.0034583 ,  0.00215372,  0.00405285, -0.00085828,  0.00428053,\n",
       "        -0.00556354, -0.00685003, -0.00590697,  0.0018979 , -0.0094387 ,\n",
       "         0.00668117,  0.00322717,  0.00258185,  0.00610396,  0.0065759 ,\n",
       "         0.00245959, -0.0097196 ,  0.00572324,  0.00972706, -0.00447755,\n",
       "        -0.00908937,  0.00093324, -0.0015826 ,  0.00056194, -0.0033917 ,\n",
       "        -0.00591095, -0.00576014, -0.0037935 ,  0.0016104 , -0.00111417,\n",
       "        -0.00323938,  0.00970042,  0.00949954, -0.00420288,  0.00105828],\n",
       "       dtype=float32),\n",
       " array([ 0.00599192,  0.00100532,  0.00875409,  0.00976201,  0.00935974,\n",
       "         0.00202746,  0.00855513, -0.00647031, -0.00064462,  0.00887595,\n",
       "        -0.00468342, -0.00302101,  0.00836356, -0.00967531, -0.00430546,\n",
       "         0.00183403, -0.00857729, -0.00306362,  0.00334355, -0.00780206,\n",
       "        -0.00086573, -0.00140614,  0.00260861, -0.00373118, -0.00185513,\n",
       "         0.00753097,  0.00364708, -0.00500581, -0.00697556,  0.00407365,\n",
       "        -0.00215342,  0.00349132,  0.00824069,  0.00794226, -0.0098096 ,\n",
       "         0.00553361, -0.007096  ,  0.00521463, -0.00391978,  0.00564434,\n",
       "         0.00492334,  0.00775931, -0.00859189,  0.00957658,  0.00393944,\n",
       "         0.00421635, -0.00698891, -0.00261936,  0.00710116,  0.0084095 ],\n",
       "       dtype=float32),\n",
       " array([-0.00518229,  0.00577825, -0.0070497 , -0.00297502,  0.00023288,\n",
       "        -0.00078597, -0.00405514, -0.00231595,  0.00048441,  0.00350722,\n",
       "         0.00818617, -0.00966074, -0.00046124,  0.00940925,  0.00475797,\n",
       "         0.00661796, -0.00754994, -0.00399259,  0.00893895,  0.0048753 ,\n",
       "         0.00994074, -0.00684551,  0.00258625, -0.00343048,  0.00603482,\n",
       "        -0.00479578, -0.00191584, -0.00451509, -0.00246861,  0.00247391,\n",
       "        -0.00012477,  0.00128608,  0.00422127, -0.00330136,  0.00260175,\n",
       "         0.0006889 ,  0.00913358, -0.00607071,  0.00427584, -0.00535246,\n",
       "         0.00462302, -0.00774266, -0.0013601 ,  0.00387053,  0.00355186,\n",
       "         0.00185673,  0.00341001, -0.00866817,  0.0076493 , -0.00151843],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Liste_T_e_i = []\n",
    "for i in range(len(T_e_i)):\n",
    "    vector = []\n",
    "    vector = model.infer_vector(T_e_i[i])\n",
    "    Liste_T_e_i.append(vector) \n",
    "Liste_T_e_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Calculate the similarity between every pair $d_{ei}$ and $t_{ei}$\n",
    "    Dont run this code bcz Liste_D_e_i length != Liste_T_e_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "similairty = []\n",
    "for i in Liste_D_e_i:\n",
    "    similairty[i] = spatial.distance.cosine(Liste_D_e_i[i], Liste_T_e_i[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbenboudiaf/.local/lib/python3.5/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el325221404179182385365141714271\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el325221404179182385365141714271_data = {\"token.table\": {\"Term\": [\"able\", \"able\", \"accept\", \"actually\", \"address\", \"agent\", \"agent\", \"agent\", \"agent\", \"agents\", \"airline\", \"airlines\", \"airways\", \"allow\", \"amaze\", \"american\", \"americanair\", \"answer\", \"appreciate\", \"aren\", \"arrival\", \"arrive\", \"arrive\", \"ask\", \"assistance\", \"attempt\", \"attendant\", \"auto\", \"automate\", \"available\", \"away\", \"bag\", \"bag\", \"baggage\", \"believe\", \"better\", \"better\", \"board\", \"board\", \"book\", \"break\", \"bring\", \"busy\", \"buy\", \"cabin\", \"call\", \"call\", \"call\", \"callback\", \"cancel\", \"card\", \"care\", \"cause\", \"chance\", \"change\", \"change\", \"charge\", \"charlotte\", \"check\", \"check\", \"check\", \"chicago\", \"claim\", \"class\", \"come\", \"communication\", \"compensate\", \"concern\", \"confirmation\", \"connect\", \"connection\", \"contact\", \"correct\", \"cost\", \"count\", \"counter\", \"country\", \"credit\", \"crew\", \"crew\", \"customer\", \"customers\", \"dallas\", \"date\", \"days\", \"days\", \"delay\", \"deliver\", \"delta\", \"depart\", \"desk\", \"destination\", \"dinner\", \"disconnect\", \"dividend\", \"dming\", \"drink\", \"earlier\", \"email\", \"email\", \"email\", \"employees\", \"expect\", \"extra\", \"extremely\", \"fail\", \"family\", \"fantastic\", \"fault\", \"feature\", \"fee\", \"feel\", \"finally\", \"fleek\", \"flight\", \"flight\", \"flightlations\", \"flightled\", \"flightr\", \"fly\", \"fly\", \"fly\", \"follow\", \"food\", \"food\", \"friend\", \"frustrate\", \"gate\", \"gate\", \"go\", \"go\", \"good\", \"good\", \"good\", \"great\", \"great\", \"guy\", \"guy\", \"half\", \"hand\", \"handle\", \"hang\", \"happen\", \"happen\", \"happy\", \"hard\", \"haven\", \"head\", \"help\", \"help\", \"hold\", \"home\", \"home\", \"home\", \"hop\", \"horrible\", \"hotel\", \"hotel\", \"hour\", \"hours\", \"http\", \"http\", \"http\", \"http\", \"human\", \"ignore\", \"info\", \"info\", \"information\", \"instead\", \"issue\", \"issue\", \"itinerary\", \"jetblue\", \"john\", \"joke\", \"know\", \"know\", \"know\", \"lack\", \"land\", \"late\", \"leave\", \"letter\", \"lie\", \"life\", \"like\", \"like\", \"link\", \"longer\", \"look\", \"look\", \"lose\", \"love\", \"love\", \"luggage\", \"maybe\", \"mean\", \"mechanical\", \"member\", \"mess\", \"message\", \"miami\", \"miles\", \"mins\", \"minute\", \"minutes\", \"miss\", \"miss\", \"miss\", \"monday\", \"money\", \"move\", \"need\", \"need\", \"need\", \"need\", \"need\", \"nice\", \"notify\", \"number\", \"offer\", \"online\", \"pass\", \"passenger\", \"passengers\", \"passengers\", \"pay\", \"people\", \"people\", \"people\", \"person\", \"philly\", \"phone\", \"phone\", \"phone\", \"pick\", \"place\", \"plan\", \"plan\", \"plane\", \"plane\", \"plane\", \"platinum\", \"policy\", \"poor\", \"possible\", \"pretty\", \"price\", \"problem\", \"problems\", \"process\", \"pull\", \"purchase\", \"question\", \"reach\", \"real\", \"rebook\", \"rebooked\", \"receive\", \"record\", \"reflight\", \"refund\", \"refuse\", \"reply\", \"reps\", \"request\", \"reschedule\", \"reservation\", \"reservations\", \"resolve\", \"respond\", \"response\", \"ridiculous\", \"right\", \"right\", \"ring\", \"rude\", \"rude\", \"say\", \"say\", \"say\", \"schedule\", \"seat\", \"second\", \"see\", \"select\", \"sell\", \"send\", \"send\", \"send\", \"seriously\", \"service\", \"shouldn\", \"show\", \"sit\", \"site\", \"situation\", \"sleep\", \"sorry\", \"southwestair\", \"southwestair\", \"southwestair\", \"speak\", \"staff\", \"standby\", \"state\", \"status\", \"stay\", \"stick\", \"stick\", \"stop\", \"strand\", \"suggestions\", \"suppose\", \"sure\", \"switch\", \"talk\", \"talk\", \"tarmac\", \"team\", \"tell\", \"tell\", \"tell\", \"terminal\", \"terrible\", \"text\", \"thank\", \"thank\", \"thank\", \"things\", \"think\", \"ticket\", \"ticket\", \"time\", \"time\", \"time\", \"time\", \"time\", \"today\", \"today\", \"tomorrow\", \"tomorrow\", \"tonight\", \"train\", \"travel\", \"travel\", \"treat\", \"trip\", \"trip\", \"try\", \"try\", \"tuesday\", \"tweet\", \"twitter\", \"unacceptable\", \"understand\", \"understand\", \"update\", \"upgrade\", \"usairways\", \"voucher\", \"wait\", \"want\", \"want\", \"want\", \"want\", \"want\", \"wasn\", \"weather\", \"weather\", \"week\", \"weeks\", \"wife\", \"work\", \"work\", \"work\", \"world\", \"worry\", \"worse\", \"worst\", \"wrong\", \"wrong\", \"yeah\", \"year\", \"yesterday\", \"yesterday\", \"zero\"], \"Topic\": [2, 8, 7, 1, 5, 2, 3, 4, 8, 8, 3, 4, 8, 3, 1, 2, 1, 2, 4, 5, 3, 4, 8, 7, 1, 5, 4, 6, 1, 6, 8, 2, 5, 8, 8, 2, 3, 2, 5, 7, 6, 5, 6, 3, 4, 1, 2, 3, 4, 2, 7, 1, 7, 3, 2, 5, 5, 8, 2, 3, 5, 2, 8, 6, 2, 3, 7, 4, 7, 2, 3, 3, 5, 3, 4, 3, 1, 7, 2, 4, 1, 4, 1, 7, 1, 3, 2, 8, 1, 4, 7, 7, 8, 3, 7, 7, 6, 2, 1, 5, 7, 1, 2, 8, 8, 2, 1, 8, 6, 5, 5, 2, 6, 6, 1, 2, 4, 2, 2, 1, 2, 3, 7, 2, 8, 3, 2, 2, 4, 1, 2, 1, 2, 4, 1, 2, 1, 2, 4, 8, 3, 1, 2, 4, 1, 5, 3, 6, 1, 2, 2, 2, 4, 8, 4, 5, 3, 8, 2, 2, 1, 4, 5, 6, 8, 8, 2, 3, 8, 5, 2, 6, 6, 2, 7, 6, 1, 2, 4, 7, 2, 2, 2, 7, 6, 4, 1, 2, 5, 7, 1, 2, 8, 1, 4, 8, 1, 4, 6, 7, 4, 1, 3, 7, 5, 5, 2, 2, 3, 8, 1, 4, 3, 1, 2, 5, 7, 8, 8, 6, 3, 3, 5, 5, 5, 2, 4, 6, 4, 5, 8, 3, 4, 1, 2, 5, 8, 6, 4, 6, 2, 4, 6, 3, 6, 8, 6, 7, 4, 1, 7, 2, 3, 7, 6, 1, 8, 2, 7, 7, 5, 7, 6, 3, 6, 4, 7, 2, 5, 5, 8, 1, 2, 4, 1, 4, 8, 1, 8, 2, 3, 6, 2, 6, 5, 5, 6, 6, 1, 7, 8, 3, 1, 5, 5, 2, 5, 4, 8, 6, 1, 2, 6, 8, 1, 5, 7, 1, 3, 2, 3, 5, 4, 5, 4, 4, 6, 1, 2, 4, 4, 1, 2, 3, 8, 6, 3, 1, 2, 7, 4, 1, 6, 7, 1, 2, 3, 5, 8, 1, 2, 1, 2, 4, 3, 1, 3, 1, 1, 4, 2, 5, 5, 8, 4, 3, 6, 7, 2, 6, 1, 3, 2, 2, 3, 4, 5, 6, 8, 2, 3, 4, 1, 1, 1, 2, 5, 3, 6, 5, 3, 3, 4, 8, 1, 1, 4, 7], \"Freq\": [0.47115716338157654, 0.5235079526901245, 0.9795235991477966, 0.9831785559654236, 0.9854199290275574, 0.693298876285553, 0.1151234358549118, 0.1279149204492569, 0.06395746022462845, 0.9952801465988159, 0.9983339905738831, 0.9954939484596252, 0.9932736754417419, 0.9908183813095093, 0.9918634295463562, 0.995663583278656, 0.9998320937156677, 0.9974526762962341, 0.9975390434265137, 0.986142098903656, 0.9744583368301392, 0.846004843711853, 0.1482052356004715, 0.9951614737510681, 0.9817323088645935, 0.9710341691970825, 0.9912901520729065, 0.9832678437232971, 0.9806869626045227, 0.9922651648521423, 0.9778808355331421, 0.8659335374832153, 0.13132640719413757, 0.9959921836853027, 0.9834829568862915, 0.3108377456665039, 0.6849942803382874, 0.6306179761886597, 0.36854296922683716, 0.9981417655944824, 0.9962146282196045, 0.9887928366661072, 0.9955454468727112, 0.9792020916938782, 0.9934467077255249, 0.7529174089431763, 0.20711320638656616, 0.03898601606488228, 0.9827319383621216, 0.9991246461868286, 0.9903152585029602, 0.98869788646698, 0.9869838953018188, 0.9837549328804016, 0.3569731116294861, 0.6418966054916382, 0.9895724654197693, 0.9900556206703186, 0.27947771549224854, 0.21186213195323944, 0.5071167945861816, 0.990238606929779, 0.9927936792373657, 0.9952602386474609, 0.9967992901802063, 0.9861186742782593, 0.9804437756538391, 0.9825214147567749, 0.9704246520996094, 0.9928367733955383, 0.9911348223686218, 0.9972774982452393, 0.9752569198608398, 0.9940128326416016, 0.9867681264877319, 0.9786372780799866, 0.9773297309875488, 0.9902099370956421, 0.7438969016075134, 0.25264424085617065, 0.9988844990730286, 0.9978769421577454, 0.9927152395248413, 0.9898658394813538, 0.9091267585754395, 0.08435196429491043, 0.998778760433197, 0.9693073034286499, 0.993922233581543, 0.9873955249786377, 0.9876905083656311, 0.9858044385910034, 0.9795261025428772, 0.9943544864654541, 0.9882160425186157, 0.9655688405036926, 0.9934751987457275, 0.9916293025016785, 0.6194174289703369, 0.11262135952711105, 0.2674757242202759, 0.9803708791732788, 0.995718240737915, 0.9910811185836792, 0.9831932783126831, 0.9962558150291443, 0.9849018454551697, 0.9811473488807678, 0.9892138838768005, 0.9722721576690674, 0.9866033792495728, 0.9860255718231201, 0.9934211373329163, 0.9841464757919312, 0.20158950984477997, 0.7981933951377869, 0.9852386713027954, 0.9992817640304565, 0.9953646063804626, 0.7222009897232056, 0.09828770905733109, 0.17520852386951447, 0.9957472085952759, 0.5061994791030884, 0.48874431848526, 0.9914146661758423, 0.9889737367630005, 0.6947813034057617, 0.3023918569087982, 0.5180491805076599, 0.4791346490383148, 0.7383304238319397, 0.18368220329284668, 0.07563384622335434, 0.9001038074493408, 0.09184733033180237, 0.995126485824585, 0.002926842775195837, 0.9934613704681396, 0.9765650629997253, 0.9906774759292603, 0.9971886873245239, 0.3908986449241638, 0.606768012046814, 0.9835980534553528, 0.9913150668144226, 0.9891132116317749, 0.9896219372749329, 0.6738927960395813, 0.32536083459854126, 0.9991956353187561, 0.8736609220504761, 0.03313886374235153, 0.09037871658802032, 0.9918535351753235, 0.9865835905075073, 0.733481228351593, 0.2631573975086212, 0.996880054473877, 0.9987241625785828, 0.49293187260627747, 0.12690657377243042, 0.13225001096725464, 0.24713386595249176, 0.9762749075889587, 0.9914919137954712, 0.43943119049072266, 0.5581963658332825, 0.984100878238678, 0.9935858249664307, 0.4629853665828705, 0.5335062742233276, 0.9939291477203369, 0.9990426898002625, 0.9897162914276123, 0.9899674654006958, 0.6057224869728088, 0.21554990112781525, 0.1773511916399002, 0.9868422746658325, 0.9957454800605774, 0.9977651834487915, 0.9986384510993958, 0.9790548086166382, 0.9905614256858826, 0.9874876737594604, 0.7682011127471924, 0.22895404696464539, 0.9813348054885864, 0.9810191988945007, 0.7665244340896606, 0.22596962749958038, 0.9974972009658813, 0.9214227199554443, 0.07818131893873215, 0.9961596131324768, 0.9919576048851013, 0.9905233979225159, 0.9922451376914978, 0.9849157333374023, 0.9819185137748718, 0.9871051907539368, 0.9910219311714172, 0.993092954158783, 0.9973805546760559, 0.9904271364212036, 0.9973865151405334, 0.6088339686393738, 0.12363295257091522, 0.2659274935722351, 0.993339478969574, 0.9911049604415894, 0.9906497001647949, 0.5277072191238403, 0.2986544370651245, 0.11515912413597107, 0.024044211953878403, 0.03416809067130089, 0.9952013492584229, 0.9910875558853149, 0.996886134147644, 0.9960988759994507, 0.9976954460144043, 0.9920626282691956, 0.9853751063346863, 0.34431642293930054, 0.6527665853500366, 0.9943153858184814, 0.22721734642982483, 0.11213324218988419, 0.6580450534820557, 0.9899654388427734, 0.9934216141700745, 0.5415497422218323, 0.3698388636112213, 0.08750651776790619, 0.993643045425415, 0.9929026961326599, 0.7715128660202026, 0.22534286975860596, 0.8685899376869202, 0.07566114515066147, 0.054476022720336914, 0.9902425408363342, 0.9846778512001038, 0.9878791570663452, 0.986707866191864, 0.9757314324378967, 0.9902411699295044, 0.9872851967811584, 0.9956774711608887, 0.9877455234527588, 0.987902045249939, 0.9888094067573547, 0.9938583970069885, 0.9903405904769897, 0.9801756739616394, 0.9955928325653076, 0.9978383779525757, 0.9897962212562561, 0.9892117381095886, 0.9815967679023743, 0.9956434369087219, 0.9789776802062988, 0.9903201460838318, 0.9839489459991455, 0.9853932857513428, 0.9902666807174683, 0.9981496930122375, 0.9949132204055786, 0.9771310687065125, 0.9840033054351807, 0.997672438621521, 0.9885743856430054, 0.8504412770271301, 0.14940184354782104, 0.9799749851226807, 0.8893511295318604, 0.10106262564659119, 0.4357962906360626, 0.11885353177785873, 0.44460025429725647, 0.9839551448822021, 0.9981930255889893, 0.9887502193450928, 0.9919990301132202, 0.9844475984573364, 0.9844562411308289, 0.6067543625831604, 0.2728358507156372, 0.11809313297271729, 0.9898346066474915, 0.9987246990203857, 0.9799603819847107, 0.9835535883903503, 0.9965653419494629, 0.9872474670410156, 0.9895356297492981, 0.9887681007385254, 0.9894722104072571, 0.5399199724197388, 0.3782327175140381, 0.08084363490343094, 0.9943826198577881, 0.9973269104957581, 0.9909084439277649, 0.9905137419700623, 0.9938883185386658, 0.9929420351982117, 0.567466676235199, 0.428752601146698, 0.9956203699111938, 0.9913808107376099, 0.9889325499534607, 0.9942218065261841, 0.9964146614074707, 0.986721932888031, 0.8883600234985352, 0.11023446172475815, 0.9826966524124146, 0.9914001226425171, 0.2799059748649597, 0.5227383375167847, 0.19649028778076172, 0.986784040927887, 0.9900692105293274, 0.9893572330474854, 0.796059787273407, 0.14753231406211853, 0.056093014776706696, 0.9900725483894348, 0.9922258853912354, 0.002300816588103771, 0.9962536096572876, 0.24414385855197906, 0.6061083674430847, 0.06316159665584564, 0.08016663789749146, 0.0060732304118573666, 0.6804611086845398, 0.318798691034317, 0.365958034992218, 0.6330084800720215, 0.9943780899047852, 0.9794469475746155, 0.7177964448928833, 0.27799054980278015, 0.9869462847709656, 0.8640090227127075, 0.13179798424243927, 0.49277418851852417, 0.5058711767196655, 0.9890314340591431, 0.9909340739250183, 0.9924337267875671, 0.9951555728912354, 0.35666006803512573, 0.6429266929626465, 0.9941593408584595, 0.9923396110534668, 0.9995304942131042, 0.9912804365158081, 0.9986903071403503, 0.45001551508903503, 0.1338653713464737, 0.25348976254463196, 0.12532077729701996, 0.03987479209899902, 0.9935199022293091, 0.7650837302207947, 0.23237499594688416, 0.989756166934967, 0.9765035510063171, 0.987920880317688, 0.6087180376052856, 0.25019344687461853, 0.1392829418182373, 0.9923672080039978, 0.9893271923065186, 0.9822056293487549, 0.9957579970359802, 0.25333890318870544, 0.7405291199684143, 0.9924753904342651, 0.989833414554596, 0.9493972063064575, 0.03828214481472969, 0.9788427352905273]}, \"R\": 30, \"plot.opts\": {\"ylab\": \"PC2\", \"xlab\": \"PC1\"}, \"topic.order\": [5, 2, 8, 1, 4, 7, 3, 6], \"tinfo\": {\"Total\": [7920.0, 5144.0, 2591.0, 1620.0, 610.0, 591.0, 610.0, 910.0, 906.0, 434.0, 877.0, 876.0, 610.0, 819.0, 371.0, 422.0, 763.0, 771.0, 395.0, 1301.0, 298.0, 637.0, 290.0, 297.0, 748.0, 443.0, 555.0, 454.0, 1035.0, 249.0, 7920.330078125, 2591.216552734375, 771.9844970703125, 637.7113647460938, 273.7696533203125, 341.66510009765625, 188.50389099121094, 169.3162841796875, 133.97598266601562, 126.42891693115234, 98.5992660522461, 89.14956665039062, 88.5411376953125, 79.26802825927734, 78.52300262451172, 73.96552276611328, 73.8925552368164, 73.41792297363281, 72.58374786376953, 68.34148406982422, 68.11725616455078, 65.63398742675781, 65.0949935913086, 64.46135711669922, 61.50040054321289, 60.419647216796875, 57.73407745361328, 56.85845947265625, 56.57517623901367, 56.02341842651367, 130.60919189453125, 213.39158630371094, 179.07090759277344, 217.75265502929688, 1301.4097900390625, 154.2167510986328, 148.42282104492188, 1035.7730712890625, 410.403564453125, 331.9443359375, 174.0272979736328, 277.6534729003906, 136.57264709472656, 225.6940460205078, 373.27630615234375, 790.2109375, 241.01539611816406, 605.669189453125, 748.5821533203125, 387.70001220703125, 234.0068817138672, 366.50445556640625, 5144.11669921875, 411.15789794921875, 284.137939453125, 346.3476257324219, 505.5224304199219, 823.2850952148438, 1620.41845703125, 910.1611938476562, 906.1868286132812, 877.7059936523438, 876.8394165039062, 819.0001831054688, 763.5484008789062, 469.0482482910156, 454.4177551269531, 402.548095703125, 314.8227844238281, 313.7993469238281, 231.53892517089844, 225.7754669189453, 220.7064208984375, 191.8894805908203, 187.7989959716797, 177.7835235595703, 166.7229766845703, 146.8577423095703, 130.4885711669922, 124.37135314941406, 117.84414672851562, 107.46012115478516, 99.835693359375, 98.96604919433594, 95.93375396728516, 88.0793685913086, 87.40235900878906, 76.06293487548828, 5144.11669921875, 660.8411865234375, 331.9365539550781, 243.66766357421875, 456.15924072265625, 555.5704956054688, 823.2850952148438, 390.8847961425781, 505.5224304199219, 284.9857177734375, 428.68829345703125, 610.8274536132812, 539.4668579101562, 1035.7730712890625, 790.2109375, 605.669189453125, 610.6902465820312, 422.3150329589844, 395.6591796875, 275.1672668457031, 177.4832000732422, 152.35061645507812, 137.5365447998047, 126.28372192382812, 121.11200714111328, 119.57929229736328, 119.29878234863281, 116.04183197021484, 115.15553283691406, 112.79611206054688, 109.95879364013672, 109.01630401611328, 103.62039184570312, 90.2528305053711, 88.76205444335938, 81.45988464355469, 78.7362060546875, 68.10639190673828, 65.7959976196289, 63.62077713012695, 63.48456573486328, 61.656192779541016, 61.30974197387695, 56.168182373046875, 55.159584045410156, 55.133155822753906, 54.389190673828125, 178.600341796875, 173.7240753173828, 168.39952087402344, 237.89942932128906, 456.15924072265625, 539.4668579101562, 443.68475341796875, 241.01539611816406, 251.53402709960938, 195.09681701660156, 193.47613525390625, 188.85096740722656, 182.65487670898438, 152.35520935058594, 143.83108520507812, 138.1884002685547, 122.05198669433594, 114.1695327758789, 113.071533203125, 103.78527069091797, 101.15576934814453, 93.37838745117188, 89.12335205078125, 88.93060302734375, 85.74684143066406, 83.54627990722656, 82.82221221923828, 82.08615112304688, 79.37342834472656, 73.4815444946289, 69.20968627929688, 68.09296417236328, 67.2153549194336, 65.8236083984375, 65.43042755126953, 62.61101150512695, 62.50922775268555, 58.0015754699707, 161.93760681152344, 261.8232421875, 278.81329345703125, 171.39993286132812, 555.5704956054688, 102.62931823730469, 351.0989990234375, 748.5821533203125, 338.882568359375, 297.68603515625, 234.43377685546875, 177.46485900878906, 150.7669219970703, 111.15911865234375, 110.48387908935547, 105.15338134765625, 103.15608978271484, 101.3598861694336, 96.61973571777344, 93.853271484375, 92.17546844482422, 84.67207336425781, 82.894287109375, 81.65330505371094, 81.33771514892578, 78.75389862060547, 78.39498901367188, 68.27435302734375, 67.90975952148438, 65.73955535888672, 62.87126541137695, 61.90254211425781, 59.875675201416016, 59.51689910888672, 49.368892669677734, 48.981571197509766, 45.50360870361328, 45.31251525878906, 44.090946197509766, 610.6902465820312, 610.8274536132812, 443.68475341796875, 244.20489501953125, 748.5821533203125, 790.2109375, 823.2850952148438, 591.0680541992188, 249.0851593017578, 163.72933959960938, 160.76197814941406, 151.86328125, 148.56236267089844, 141.0807342529297, 140.92713928222656, 127.98998260498047, 124.2337417602539, 119.85061645507812, 112.18101501464844, 108.04586791992188, 102.08958435058594, 94.58087921142578, 84.11811065673828, 79.2137222290039, 78.79046630859375, 78.4764175415039, 74.70510864257812, 72.20819854736328, 70.76325988769531, 68.44660186767578, 68.05787658691406, 67.52363586425781, 62.55754089355469, 60.807403564453125, 57.59775161743164, 53.83729934692383, 52.83766174316406, 326.14422607421875, 454.3407287597656, 748.5821533203125, 213.0880584716797, 610.1337890625, 290.62823486328125, 210.9116668701172, 434.6282958984375, 195.8328399658203, 175.21018981933594, 127.24574279785156, 126.61261749267578, 124.26800537109375, 99.30664825439453, 97.19644165039062, 92.20008850097656, 90.8801498413086, 78.14138793945312, 69.78089141845703, 68.40059661865234, 65.19989013671875, 64.3477783203125, 62.892799377441406, 62.21672439575195, 61.16088104248047, 60.91891860961914, 56.57332229614258, 46.440547943115234, 44.95104217529297, 44.457183837890625, 43.28002166748047, 43.04463195800781, 41.4889030456543, 42.46201705932617, 213.0880584716797, 284.137939453125, 245.5688934326172, 1301.4097900390625, 371.4264221191406, 298.7477111816406, 233.10020446777344, 222.2484588623047, 194.7806396484375, 150.7232666015625, 133.8674774169922, 130.160888671875, 120.08871459960938, 115.39872741699219, 111.71013641357422, 106.71781921386719, 91.66053771972656, 84.84371948242188, 77.87467956542969, 73.20919799804688, 68.08245086669922, 65.29441833496094, 62.470062255859375, 58.38519287109375, 58.289310455322266, 52.696434020996094, 51.43763732910156, 50.14680480957031, 47.903099060058594, 44.361576080322266, 44.031883239746094, 43.87866973876953, 42.87787628173828, 42.71794891357422, 338.882568359375, 133.71334838867188, 428.68829345703125, 114.57933807373047, 178.600341796875], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.280400037765503, 1.2800999879837036, 1.2791999578475952, 1.2790000438690186, 1.276900053024292, 1.2762999534606934, 1.2753000259399414, 1.2747999429702759, 1.2732000350952148, 1.2727999687194824, 1.2705999612808228, 1.2696000337600708, 1.2695000171661377, 1.2682000398635864, 1.2681000232696533, 1.267300009727478, 1.267300009727478, 1.267199993133545, 1.2670999765396118, 1.2661999464035034, 1.2661999464035034, 1.2655999660491943, 1.2654999494552612, 1.2653000354766846, 1.2646000385284424, 1.264299988746643, 1.2635999917984009, 1.2632999420166016, 1.263200044631958, 1.263100028038025, 1.2309000492095947, 1.1861000061035156, 1.1956000328063965, 1.1776000261306763, 1.0525000095367432, 1.1611000299453735, 1.1648999452590942, 0.8858000040054321, 0.9973999857902527, 1.0162999629974365, 1.1159000396728516, 0.9775000214576721, 1.1324000358581543, 1.0174000263214111, 0.8948000073432922, 0.6401000022888184, 0.949999988079071, 0.6672000288963318, 0.572700023651123, 0.7843000292778015, 0.9531999826431274, 0.7792999744415283, -0.3208000063896179, 0.6205000281333923, 0.8016999959945679, 0.6621999740600586, 0.274399995803833, -0.12970000505447388, 1.3253999948501587, 1.3249000310897827, 1.3249000310897827, 1.3249000310897827, 1.3249000310897827, 1.3248000144958496, 1.3246999979019165, 1.3240000009536743, 1.3238999843597412, 1.3236000537872314, 1.3229999542236328, 1.3229999542236328, 1.3219000101089478, 1.3217999935150146, 1.3216999769210815, 1.3209999799728394, 1.3208999633789062, 1.3207000494003296, 1.3202999830245972, 1.3194999694824219, 1.3186999559402466, 1.3184000253677368, 1.3178999423980713, 1.317199945449829, 1.316499948501587, 1.3164000511169434, 1.316100001335144, 1.3151999711990356, 1.3150999546051025, 1.3135000467300415, 1.100600004196167, 1.1852999925613403, 1.1907000541687012, 1.1806999444961548, 1.0591000318527222, 0.963100016117096, 0.8260999917984009, 0.9605000019073486, 0.8682000041007996, 1.0289000272750854, 0.8287000060081482, 0.6190999746322632, 0.6783000230789185, 0.2029999941587448, 0.11829999834299088, 0.3328000009059906, 0.29679998755455017, 2.3919999599456787, 2.3917999267578125, 2.3907999992370605, 2.3889000415802, 2.3880999088287354, 2.387399911880493, 2.3868000507354736, 2.3864998817443848, 2.386399984359741, 2.386399984359741, 2.386199951171875, 2.3861000537872314, 2.3859000205993652, 2.385699987411499, 2.385699987411499, 2.385200023651123, 2.3838999271392822, 2.383699893951416, 2.3828001022338867, 2.3824000358581543, 2.380500078201294, 2.380000114440918, 2.379499912261963, 2.379499912261963, 2.378999948501587, 2.378999948501587, 2.3775999546051025, 2.3773000240325928, 2.3773000240325928, 2.377000093460083, 2.0820999145507812, 2.0153000354766846, 1.811900019645691, 1.545799970626831, 0.9351000189781189, 0.765999972820282, 0.8443999886512756, 1.1139999628067017, 2.4391000270843506, 2.437999963760376, 2.437999963760376, 2.4377999305725098, 2.437700033187866, 2.4367001056671143, 2.436300039291382, 2.436000108718872, 2.4351999759674072, 2.4346001148223877, 2.4346001148223877, 2.433799982070923, 2.4335999488830566, 2.432800054550171, 2.432300090789795, 2.432300090789795, 2.4319000244140625, 2.4316000938415527, 2.43149995803833, 2.4314000606536865, 2.4310998916625977, 2.4300999641418457, 2.42930006980896, 2.4291000366210938, 2.4289000034332275, 2.4286000728607178, 2.4286000728607178, 2.4279000759124756, 2.4279000759124756, 2.4267001152038574, 2.275399923324585, 2.184499979019165, 2.016400098800659, 1.9414000511169434, 1.2466000318527222, 2.1426000595092773, 1.0667999982833862, 0.38089999556541443, 0.9639999866485596, 2.6006999015808105, 2.599900007247925, 2.598599910736084, 2.5977001190185547, 2.5954999923706055, 2.595400094985962, 2.5950000286102295, 2.5947999954223633, 2.5947000980377197, 2.5941998958587646, 2.593899965286255, 2.5938000679016113, 2.592900037765503, 2.592600107192993, 2.5924999713897705, 2.592400074005127, 2.5920000076293945, 2.5920000076293945, 2.5901999473571777, 2.590100049972534, 2.5896999835968018, 2.5889999866485596, 2.5887999534606934, 2.5882999897003174, 2.588200092315674, 2.5850000381469727, 2.5848000049591064, 2.5833001136779785, 2.5833001136779785, 2.58270001411438, 2.1596999168395996, 1.9215999841690063, 1.9229999780654907, 1.601699948310852, 0.5853000283241272, 0.44429999589920044, 0.0812000036239624, 2.6414999961853027, 2.639400005340576, 2.637500047683716, 2.637399911880493, 2.63700008392334, 2.636899948120117, 2.6366000175476074, 2.6366000175476074, 2.6359000205993652, 2.635699987411499, 2.6354000568389893, 2.6349000930786133, 2.6345999240875244, 2.6340999603271484, 2.6333999633789062, 2.632200002670288, 2.631500005722046, 2.6314001083374023, 2.6314001083374023, 2.6308000087738037, 2.6303999423980713, 2.6301000118255615, 2.629699945449829, 2.6296000480651855, 2.629499912261963, 2.6284000873565674, 2.628000020980835, 2.6270999908447266, 2.625999927520752, 2.625699996948242, 2.015199899673462, 1.833899974822998, 1.2426999807357788, 1.608199954032898, 2.6786999702453613, 2.677000045776367, 2.675800085067749, 2.675800085067749, 2.67549991607666, 2.6749000549316406, 2.6728999614715576, 2.6728999614715576, 2.672800064086914, 2.6709001064300537, 2.6707000732421875, 2.6702001094818115, 2.6700000762939453, 2.668299913406372, 2.6668999195098877, 2.666599988937378, 2.6659998893737793, 2.665800094604492, 2.6654000282287598, 2.665299892425537, 2.6649999618530273, 2.6649999618530273, 2.663800001144409, 2.660099983215332, 2.6594998836517334, 2.65910005569458, 2.6586999893188477, 2.6584999561309814, 2.6577000617980957, 2.6572999954223633, 2.2356998920440674, 1.3553999662399292, 1.3866000175476074, -0.20149999856948853, 2.7355000972747803, 2.7348999977111816, 2.7339999675750732, 2.733799934387207, 2.7332000732421875, 2.731800079345703, 2.7309999465942383, 2.730799913406372, 2.7302000522613525, 2.7298998832702637, 2.729599952697754, 2.729300022125244, 2.727799892425537, 2.7269999980926514, 2.7260000705718994, 2.7251999378204346, 2.7242000102996826, 2.723599910736084, 2.7230000495910645, 2.72189998626709, 2.72189998626709, 2.7202000617980957, 2.7197000980377197, 2.7193000316619873, 2.7183001041412354, 2.7167999744415283, 2.716599941253662, 2.7165000438690186, 2.7160000801086426, 2.7160000801086426, 2.3187999725341797, 2.0922000408172607, 1.4119999408721924, 2.0151000022888184, 1.4047000408172607], \"Term\": [\"americanair\", \"flight\", \"usairways\", \"cancel\", \"book\", \"seat\", \"change\", \"hours\", \"wait\", \"ticket\", \"hold\", \"jetblue\", \"try\", \"delay\", \"luggage\", \"number\", \"flightled\", \"service\", \"airline\", \"thank\", \"lose\", \"customer\", \"rebooked\", \"online\", \"http\", \"check\", \"gate\", \"say\", \"help\", \"refund\", \"americanair\", \"usairways\", \"service\", \"customer\", \"hang\", \"guy\", \"staff\", \"think\", \"dallas\", \"care\", \"delta\", \"message\", \"status\", \"respond\", \"monday\", \"treat\", \"wife\", \"automate\", \"maybe\", \"employees\", \"happy\", \"reach\", \"actually\", \"country\", \"amaze\", \"weeks\", \"problem\", \"family\", \"year\", \"assistance\", \"yesterday\", \"days\", \"love\", \"great\", \"thank\", \"talk\", \"rude\", \"help\", \"call\", \"like\", \"right\", \"good\", \"trip\", \"look\", \"today\", \"need\", \"travel\", \"phone\", \"http\", \"work\", \"fly\", \"know\", \"flight\", \"go\", \"email\", \"southwestair\", \"tomorrow\", \"time\", \"cancel\", \"hours\", \"wait\", \"hold\", \"jetblue\", \"delay\", \"flightled\", \"late\", \"hour\", \"leave\", \"minutes\", \"answer\", \"response\", \"sit\", \"come\", \"flightr\", \"land\", \"rebook\", \"american\", \"update\", \"fail\", \"frustrate\", \"connect\", \"expect\", \"earlier\", \"chicago\", \"reschedule\", \"process\", \"schedule\", \"feel\", \"flight\", \"plane\", \"home\", \"bag\", \"weather\", \"gate\", \"time\", \"agent\", \"tomorrow\", \"crew\", \"miss\", \"try\", \"tell\", \"help\", \"need\", \"phone\", \"change\", \"number\", \"airline\", \"worst\", \"contact\", \"connection\", \"offer\", \"seriously\", \"allow\", \"unacceptable\", \"haven\", \"miami\", \"person\", \"stay\", \"voucher\", \"handle\", \"cost\", \"communication\", \"friend\", \"disconnect\", \"move\", \"chance\", \"pull\", \"platinum\", \"world\", \"text\", \"counter\", \"buy\", \"refuse\", \"train\", \"arrival\", \"hotel\", \"better\", \"info\", \"stick\", \"weather\", \"tell\", \"check\", \"travel\", \"customers\", \"tonight\", \"appreciate\", \"airlines\", \"sure\", \"money\", \"suppose\", \"team\", \"strand\", \"week\", \"mean\", \"twitter\", \"ridiculous\", \"flightlations\", \"depart\", \"situation\", \"attendant\", \"half\", \"things\", \"count\", \"tarmac\", \"cabin\", \"concern\", \"reps\", \"mess\", \"life\", \"philly\", \"price\", \"hop\", \"callback\", \"arrive\", \"plan\", \"passengers\", \"happen\", \"gate\", \"wrong\", \"want\", \"http\", \"people\", \"online\", \"reservation\", \"mins\", \"reservations\", \"charge\", \"stop\", \"tuesday\", \"bring\", \"horrible\", \"instead\", \"standby\", \"site\", \"pass\", \"record\", \"see\", \"show\", \"minute\", \"worse\", \"link\", \"fee\", \"second\", \"aren\", \"address\", \"passenger\", \"hard\", \"feature\", \"shouldn\", \"suggestions\", \"attempt\", \"correct\", \"change\", \"try\", \"check\", \"board\", \"http\", \"need\", \"time\", \"seat\", \"refund\", \"busy\", \"class\", \"pay\", \"break\", \"upgrade\", \"finally\", \"available\", \"terrible\", \"place\", \"sorry\", \"reply\", \"worry\", \"question\", \"possible\", \"policy\", \"joke\", \"itinerary\", \"lie\", \"auto\", \"fault\", \"drink\", \"sell\", \"mechanical\", \"notify\", \"switch\", \"head\", \"select\", \"fleek\", \"issue\", \"say\", \"http\", \"understand\", \"book\", \"rebooked\", \"problems\", \"ticket\", \"follow\", \"miles\", \"credit\", \"ask\", \"receive\", \"lack\", \"desk\", \"cause\", \"card\", \"request\", \"purchase\", \"accept\", \"reflight\", \"letter\", \"destination\", \"compensate\", \"longer\", \"member\", \"date\", \"state\", \"zero\", \"john\", \"confirmation\", \"pretty\", \"dividend\", \"dming\", \"understand\", \"email\", \"send\", \"thank\", \"luggage\", \"lose\", \"agents\", \"speak\", \"baggage\", \"nice\", \"wasn\", \"extra\", \"tweet\", \"poor\", \"pick\", \"airways\", \"claim\", \"charlotte\", \"sleep\", \"believe\", \"information\", \"real\", \"yeah\", \"human\", \"away\", \"terminal\", \"ignore\", \"resolve\", \"fantastic\", \"deliver\", \"hand\", \"ring\", \"dinner\", \"extremely\", \"people\", \"able\", \"miss\", \"food\", \"hotel\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.3732999563217163, -2.4909000396728516, -3.7026000022888184, -3.8940000534057617, -4.741600036621094, -4.520699977874756, -5.116399765014648, -5.224299907684326, -5.460000038146973, -5.518400192260742, -5.769199848175049, -5.870999813079834, -5.877900123596191, -5.989799976348877, -5.9994001388549805, -6.059899806976318, -6.0609002113342285, -6.067500114440918, -6.078999996185303, -6.140100002288818, -6.143400192260742, -6.181099891662598, -6.189499855041504, -6.19950008392334, -6.247200012207031, -6.265200138092041, -6.311399936676025, -6.327000141143799, -6.331999778747559, -6.3420000076293945, -5.527699947357178, -5.081600189208984, -5.247399806976318, -5.069900035858154, -3.4072000980377197, -5.431399822235107, -5.46589994430542, -3.8022000789642334, -4.616300106048584, -4.809599876403809, -5.3557000160217285, -5.026899814605713, -5.581600189208984, -5.194300174713135, -4.813700199127197, -4.318399906158447, -5.196000099182129, -4.557400226593018, -4.440000057220459, -4.886300086975098, -5.2221999168396, -4.9475998878479, -3.4059998989105225, -4.991300106048584, -5.179699897766113, -5.121200084686279, -5.130899906158447, -5.047299861907959, -2.9149999618530273, -3.492300033569336, -3.4967000484466553, -3.528599977493286, -3.529599905014038, -3.597899913787842, -3.668100118637085, -4.156199932098389, -4.187900066375732, -4.3094000816345215, -4.5559000968933105, -4.559100151062012, -4.864200115203857, -4.889500141143799, -4.912300109863281, -5.0528998374938965, -5.07450008392334, -5.1296000480651855, -5.194200038909912, -5.321899890899658, -5.440800189971924, -5.489200115203857, -5.543499946594238, -5.636600017547607, -5.7108001708984375, -5.719699859619141, -5.751100063323975, -5.837500095367432, -5.845200061798096, -5.985799789428711, -1.9846999645233154, -3.9521000385284424, -4.635200023651123, -4.9542999267578125, -4.44890022277832, -4.347799777984619, -4.091400146484375, -4.701900005340576, -4.537099838256836, -4.94950008392334, -4.741399765014648, -4.59689998626709, -4.6620001792907715, -4.485000133514404, -4.8403000831604, -4.89169979095459, -4.91949987411499, -3.1930999755859375, -3.258500099182129, -3.6226999759674072, -4.063000202178955, -4.216599941253662, -4.319499969482422, -4.4054999351501465, -4.4475998878479, -4.4604997634887695, -4.462800025939941, -4.490699768066406, -4.498499870300293, -4.5192999839782715, -4.545000076293945, -4.553699970245361, -4.604899883270264, -4.7444000244140625, -4.761199951171875, -4.8480000495910645, -4.882400035858154, -5.029300212860107, -5.064300060272217, -5.098400115966797, -5.100500106811523, -5.130199909210205, -5.135900020599365, -5.224899768829346, -5.243299961090088, -5.243800163269043, -5.257699966430664, -4.36359977722168, -4.458099842071533, -4.692599773406982, -4.6132001876831055, -4.57289981842041, -4.5742998123168945, -4.691299915313721, -5.0320000648498535, -3.6642000675201416, -3.919300079345703, -3.9277000427246094, -3.9519999027252197, -3.985599994659424, -4.168000221252441, -4.225900173187256, -4.266200065612793, -4.391200065612793, -4.458499908447266, -4.468299865722656, -4.554699897766113, -4.580599784851074, -4.661399841308594, -4.708499908447266, -4.710700035095215, -4.747499942779541, -4.773799896240234, -4.782599925994873, -4.791600227355957, -4.8256001472473145, -4.90369987487793, -4.964399814605713, -4.980899810791016, -4.99399995803833, -5.015200138092041, -5.021299839019775, -5.065999984741211, -5.067599773406982, -5.143700122833252, -4.2683000564575195, -3.878700017929077, -3.9839000701904297, -4.545499801635742, -4.064300060272217, -4.857100009918213, -4.703000068664551, -4.631800174713135, -4.84119987487793, -3.3341000080108643, -3.5738000869750977, -3.8534998893737793, -4.017399787902832, -4.324399948120117, -4.33050012588501, -4.38040018081665, -4.399799823760986, -4.417500019073486, -4.465799808502197, -4.495200157165527, -4.513400077819824, -4.5991997718811035, -4.62060022354126, -4.635900020599365, -4.639800071716309, -4.672500133514404, -4.67710018157959, -4.8171000480651855, -4.822500228881836, -4.855500221252441, -4.900700092315674, -4.916500091552734, -4.950300216674805, -4.956399917602539, -5.146599769592285, -5.154600143432617, -5.229700088500977, -5.234000205993652, -5.261899948120117, -3.0566000938415527, -3.2943999767303467, -3.6126999855041504, -4.531199932098389, -4.4274001121521, -4.5142998695373535, -4.836400032043457, -2.6073999404907227, -3.4737000465393066, -3.89520001411438, -3.913599967956543, -3.9707999229431152, -3.9928998947143555, -4.044899940490723, -4.046000003814697, -4.14300012588501, -4.172999858856201, -4.209199905395508, -4.2758002281188965, -4.313700199127197, -4.3709001541137695, -4.447999954223633, -4.566400051116943, -4.627200126647949, -4.632599830627441, -4.63670015335083, -4.686500072479248, -4.720900058746338, -4.741399765014648, -4.775100231170654, -4.780900001525879, -4.788899898529053, -4.866399765014648, -4.895100116729736, -4.950200080871582, -5.018899917602539, -5.037899971008301, -3.8282999992370605, -3.678100109100342, -3.7699999809265137, -4.660900115966797, -2.5385000705718994, -3.2818000316619873, -3.603600025177002, -2.880500078201294, -3.678100109100342, -3.789900064468384, -4.111800193786621, -4.116799831390381, -4.1356000900268555, -4.361800193786621, -4.383399963378906, -4.436699867248535, -4.451300144195557, -4.604000091552734, -4.718599796295166, -4.738900184631348, -4.787399768829346, -4.80079984664917, -4.823999881744385, -4.835000038146973, -4.85230016708374, -4.856400012969971, -4.93149995803833, -5.132500171661377, -5.165800094604492, -5.177199840545654, -5.204500198364258, -5.210100173950195, -5.247700214385986, -5.224899768829346, -4.0335001945495605, -4.625999927520752, -4.740699768066406, -4.661099910736084, -2.9779999256134033, -3.1963000297546387, -3.4453999996185303, -3.4932000637054443, -3.625699996948242, -3.8835999965667725, -4.002999782562256, -4.031199932098389, -4.112400054931641, -4.152500152587891, -4.185299873352051, -4.231400012969971, -4.384900093078613, -4.463099956512451, -4.549799919128418, -4.612299919128418, -4.6859002113342285, -4.728300094604492, -4.773200035095215, -4.841899871826172, -4.843500137329102, -4.946100234985352, -4.970699787139893, -4.996600151062012, -5.043399810791016, -5.121699810028076, -5.129300117492676, -5.132900238037109, -5.156499862670898, -5.160299777984619, -3.4863998889923096, -4.642899990081787, -4.158100128173828, -4.874499797821045, -5.040999889373779], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"Freq\": [7920.0, 5144.0, 2591.0, 1620.0, 610.0, 591.0, 610.0, 910.0, 906.0, 434.0, 877.0, 876.0, 610.0, 819.0, 371.0, 422.0, 763.0, 771.0, 395.0, 1301.0, 298.0, 637.0, 290.0, 297.0, 748.0, 443.0, 555.0, 454.0, 1035.0, 249.0, 7919.3564453125, 2590.245361328125, 771.0174560546875, 636.744384765625, 272.8016357421875, 340.24072265625, 187.5362548828125, 168.34901428222656, 133.00767517089844, 125.46184539794922, 97.63195037841797, 88.18242645263672, 87.5737533569336, 78.30079650878906, 77.55413055419922, 72.99779510498047, 72.92539978027344, 72.45015716552734, 71.61632537841797, 67.37430572509766, 67.1501235961914, 64.66648864746094, 64.12767028808594, 63.49169921875, 60.53330993652344, 59.452239990234375, 56.76713562011719, 55.891197204589844, 55.60810089111328, 55.05558395385742, 124.2913589477539, 194.17726135253906, 164.50454711914062, 196.46949768066406, 1036.068603515625, 136.85865783691406, 132.2238006591797, 697.9830322265625, 309.2085876464844, 254.8837432861328, 147.6221923828125, 205.08482360839844, 117.77410125732422, 173.48403930664062, 253.8264923095703, 416.51806640625, 173.18582153320312, 327.99627685546875, 368.8355407714844, 236.04949951171875, 168.69790649414062, 222.0244598388672, 1037.268310546875, 212.51519775390625, 176.0396270751953, 186.62966918945312, 184.84320068359375, 200.95233154296875, 1619.475341796875, 909.2178955078125, 905.24365234375, 876.7626342773438, 875.892822265625, 818.0570678710938, 762.6052856445312, 468.105224609375, 453.4747009277344, 401.6041564941406, 313.8798828125, 312.8553466796875, 230.5947265625, 224.832275390625, 219.7630157470703, 190.94644165039062, 186.85501098632812, 176.84019470214844, 165.77679443359375, 145.91424560546875, 129.5449676513672, 123.42729949951172, 116.90107727050781, 106.5163803100586, 98.8924560546875, 98.02239990234375, 94.99088287353516, 87.13151550292969, 86.45917510986328, 75.11865234375, 4106.0, 574.102783203125, 289.9344482421875, 210.7228240966797, 349.3221740722656, 386.4861755371094, 499.4139404296875, 271.2274169921875, 319.8310852050781, 211.73977661132812, 260.71832275390625, 301.2529296875, 282.2817077636719, 336.9419860839844, 236.1821746826172, 224.33938598632812, 218.2011260986328, 421.3929748535156, 394.7375183105469, 274.2457275390625, 176.56092834472656, 151.428955078125, 136.61480712890625, 125.3617172241211, 120.18999481201172, 118.6573257446289, 118.37678527832031, 115.11961364746094, 114.23368835449219, 111.87409973144531, 109.03701782226562, 108.0942611694336, 102.6983413696289, 89.33009338378906, 87.8404312133789, 80.53804779052734, 77.81414794921875, 67.18486022949219, 64.87360382080078, 62.69850540161133, 62.562767028808594, 60.73099136352539, 60.38762283325195, 55.24592208862305, 54.237762451171875, 54.211544036865234, 53.46632766723633, 130.72828674316406, 118.9393081665039, 94.07540130615234, 101.84961700439453, 106.03385925292969, 105.8938217163086, 94.1960220336914, 67.00326538085938, 250.61207580566406, 194.17498779296875, 192.5543975830078, 187.92909240722656, 181.73326110839844, 151.43338012695312, 142.90928649902344, 137.26654052734375, 121.13016510009766, 113.247802734375, 112.14969635009766, 102.86357879638672, 100.23410034179688, 92.45578002929688, 88.20154571533203, 88.00862121582031, 84.82516479492188, 82.62443542480469, 81.90043640136719, 81.16429901123047, 78.45164489746094, 72.55783081054688, 68.2877426147461, 67.16962432861328, 66.29344177246094, 64.90164184570312, 64.50860595703125, 61.68887710571289, 61.58760452270508, 57.077720642089844, 136.98025512695312, 202.2371063232422, 182.03729248046875, 103.8171615600586, 167.9783935546875, 76.0191879272461, 88.68921661376953, 95.23262786865234, 77.24055480957031, 296.7646789550781, 233.51243591308594, 176.54336547851562, 149.84532165527344, 110.23785400390625, 109.56245422363281, 104.23076629638672, 102.23460388183594, 100.43839263916016, 95.69834899902344, 92.93158721923828, 91.25402069091797, 83.75096893310547, 81.97295379638672, 80.73164367675781, 80.4162826538086, 77.8319320678711, 77.47319030761719, 67.3531494140625, 66.98811340332031, 64.818115234375, 61.94976806640625, 60.981201171875, 58.953983306884766, 58.5954704284668, 48.446659088134766, 48.05978775024414, 44.58119583129883, 44.39080810546875, 43.16938400268555, 391.6869201660156, 308.7721252441406, 224.60548400878906, 89.64427947998047, 99.45219421386719, 91.1742935180664, 66.06517791748047, 590.16064453125, 248.177490234375, 162.82086181640625, 159.8546142578125, 150.9558563232422, 147.65472412109375, 140.1734161376953, 140.01966857910156, 127.08246612548828, 123.326171875, 118.94284057617188, 111.2735824584961, 107.13817596435547, 101.18206787109375, 93.6734848022461, 83.2104263305664, 78.30624389648438, 77.8829116821289, 77.56730651855469, 73.79702758789062, 71.29879760742188, 69.85533142089844, 67.53917694091797, 67.15036010742188, 66.6163330078125, 61.649681091308594, 59.89997863769531, 56.69028091430664, 52.92924118041992, 51.92982482910156, 174.07273864746094, 202.29579162597656, 184.53187561035156, 75.70791625976562, 609.2103271484375, 289.7041931152344, 209.98846435546875, 432.7231140136719, 194.9093780517578, 174.2869415283203, 126.32252502441406, 125.68916320800781, 123.3445816040039, 98.38272857666016, 96.27273559570312, 91.2763900756836, 89.95692443847656, 77.21783447265625, 68.85736083984375, 67.47595977783203, 64.27684783935547, 63.42424011230469, 61.96946716308594, 61.29253387451172, 60.237430572509766, 59.995567321777344, 55.6500129699707, 45.51667022705078, 44.027381896972656, 43.527828216552734, 42.356849670410156, 42.12085723876953, 40.56440353393555, 41.49999237060547, 136.61106872558594, 75.5403060913086, 67.35143280029297, 72.93080139160156, 370.4940185546875, 297.8154602050781, 232.16738891601562, 221.31610107421875, 193.8485107421875, 149.7908477783203, 132.9347686767578, 129.2281036376953, 119.15647888183594, 114.46624755859375, 110.77779388427734, 105.78540802001953, 90.7285385131836, 83.91107177734375, 76.94224548339844, 72.2766342163086, 67.15001678466797, 64.36215209960938, 61.53765869140625, 57.45307159423828, 57.35689163208008, 51.76393127441406, 50.505332946777344, 49.214569091796875, 46.967960357666016, 43.42948913574219, 43.09904861450195, 42.94404602050781, 41.94234848022461, 41.785491943359375, 222.82228088378906, 70.09732055664062, 113.83220672607422, 55.607425689697266, 47.08037567138672]}, \"lambda.step\": 0.01, \"mdsDat\": {\"y\": [-0.1292728268065786, 0.08377684562280527, 0.16786486865035924, 0.13139696460348016, 0.01761782017073514, -0.03835930922788526, -0.2896150953457022, 0.056590732332786116], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"x\": [-0.31841027741818684, -0.2769529972126847, 0.03222143245023141, 0.09585494368547907, 0.04808868334379652, 0.13476968804676848, 0.13788620464509693, 0.14654232245949916], \"Freq\": [27.79009246826172, 26.55445098876953, 9.12505054473877, 8.692224502563477, 7.399105072021484, 7.114550590515137, 6.854761600494385, 6.469763278961182]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el325221404179182385365141714271\", ldavis_el325221404179182385365141714271_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el325221404179182385365141714271\", ldavis_el325221404179182385365141714271_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el325221404179182385365141714271\", ldavis_el325221404179182385365141714271_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "4      27.790092        1       1 -0.318410 -0.129273\n",
       "1      26.554451        1       2 -0.276953  0.083777\n",
       "7       9.125051        1       3  0.032221  0.167865\n",
       "0       8.692225        1       4  0.095855  0.131397\n",
       "3       7.399105        1       5  0.048089  0.017618\n",
       "6       7.114551        1       6  0.134770 -0.038359\n",
       "2       6.854762        1       7  0.137886 -0.289615\n",
       "5       6.469763        1       8  0.146542  0.056591, topic_info=     Category         Freq         Term        Total  loglift  logprob\n",
       "717   Default  7920.000000  americanair  7920.000000  30.0000  30.0000\n",
       "6631  Default  5144.000000       flight  5144.000000  29.0000  29.0000\n",
       "6760  Default  2591.000000    usairways  2591.000000  28.0000  28.0000\n",
       "8864  Default  1620.000000       cancel  1620.000000  27.0000  27.0000\n",
       "440   Default   610.000000         book   610.000000  26.0000  26.0000\n",
       "6371  Default   591.000000         seat   591.000000  25.0000  25.0000\n",
       "153   Default   610.000000       change   610.000000  24.0000  24.0000\n",
       "449   Default   910.000000        hours   910.000000  23.0000  23.0000\n",
       "6094  Default   906.000000         wait   906.000000  22.0000  22.0000\n",
       "936   Default   434.000000       ticket   434.000000  21.0000  21.0000\n",
       "8919  Default   877.000000         hold   877.000000  20.0000  20.0000\n",
       "656   Default   876.000000      jetblue   876.000000  19.0000  19.0000\n",
       "6267  Default   610.000000          try   610.000000  18.0000  18.0000\n",
       "8153  Default   819.000000        delay   819.000000  17.0000  17.0000\n",
       "5475  Default   371.000000      luggage   371.000000  16.0000  16.0000\n",
       "5025  Default   422.000000       number   422.000000  15.0000  15.0000\n",
       "9236  Default   763.000000    flightled   763.000000  14.0000  14.0000\n",
       "4098  Default   771.000000      service   771.000000  13.0000  13.0000\n",
       "9704  Default   395.000000      airline   395.000000  12.0000  12.0000\n",
       "6691  Default  1301.000000        thank  1301.000000  11.0000  11.0000\n",
       "4948  Default   298.000000         lose   298.000000  10.0000  10.0000\n",
       "3150  Default   637.000000     customer   637.000000   9.0000   9.0000\n",
       "2166  Default   290.000000     rebooked   290.000000   8.0000   8.0000\n",
       "1000  Default   297.000000       online   297.000000   7.0000   7.0000\n",
       "650   Default   748.000000         http   748.000000   6.0000   6.0000\n",
       "5813  Default   443.000000        check   443.000000   5.0000   5.0000\n",
       "7539  Default   555.000000         gate   555.000000   4.0000   4.0000\n",
       "3851  Default   454.000000          say   454.000000   3.0000   3.0000\n",
       "2728  Default  1035.000000         help  1035.000000   2.0000   2.0000\n",
       "6807  Default   249.000000       refund   249.000000   1.0000   1.0000\n",
       "...       ...          ...          ...          ...      ...      ...\n",
       "2017   Topic8   149.790848         nice   150.723267   2.7318  -3.8836\n",
       "7002   Topic8   132.934769         wasn   133.867477   2.7310  -4.0030\n",
       "3832   Topic8   129.228104        extra   130.160889   2.7308  -4.0312\n",
       "739    Topic8   119.156479        tweet   120.088715   2.7302  -4.1124\n",
       "1462   Topic8   114.466248         poor   115.398727   2.7299  -4.1525\n",
       "1496   Topic8   110.777794         pick   111.710136   2.7296  -4.1853\n",
       "993    Topic8   105.785408      airways   106.717819   2.7293  -4.2314\n",
       "7722   Topic8    90.728539        claim    91.660538   2.7278  -4.3849\n",
       "3437   Topic8    83.911072    charlotte    84.843719   2.7270  -4.4631\n",
       "7420   Topic8    76.942245        sleep    77.874680   2.7260  -4.5498\n",
       "9604   Topic8    72.276634      believe    73.209198   2.7252  -4.6123\n",
       "126    Topic8    67.150017  information    68.082451   2.7242  -4.6859\n",
       "3159   Topic8    64.362152         real    65.294418   2.7236  -4.7283\n",
       "3865   Topic8    61.537659         yeah    62.470062   2.7230  -4.7732\n",
       "8116   Topic8    57.453072        human    58.385193   2.7219  -4.8419\n",
       "5328   Topic8    57.356892         away    58.289310   2.7219  -4.8435\n",
       "4263   Topic8    51.763931     terminal    52.696434   2.7202  -4.9461\n",
       "9278   Topic8    50.505333       ignore    51.437637   2.7197  -4.9707\n",
       "4181   Topic8    49.214569      resolve    50.146805   2.7193  -4.9966\n",
       "8589   Topic8    46.967960    fantastic    47.903099   2.7183  -5.0434\n",
       "8101   Topic8    43.429489      deliver    44.361576   2.7168  -5.1217\n",
       "1113   Topic8    43.099049         hand    44.031883   2.7166  -5.1293\n",
       "5099   Topic8    42.944046         ring    43.878670   2.7165  -5.1329\n",
       "9463   Topic8    41.942348       dinner    42.877876   2.7160  -5.1565\n",
       "2741   Topic8    41.785492    extremely    42.717949   2.7160  -5.1603\n",
       "5239   Topic8   222.822281       people   338.882568   2.3188  -3.4864\n",
       "5476   Topic8    70.097321         able   133.713348   2.0922  -4.6429\n",
       "512    Topic8   113.832207         miss   428.688293   1.4120  -4.1581\n",
       "2030   Topic8    55.607426         food   114.579338   2.0151  -4.8745\n",
       "8786   Topic8    47.080376        hotel   178.600342   1.4047  -5.0410\n",
       "\n",
       "[352 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "5476      2  0.471157         able\n",
       "5476      8  0.523508         able\n",
       "5788      7  0.979524       accept\n",
       "5797      1  0.983179     actually\n",
       "2732      5  0.985420      address\n",
       "7050      2  0.693299        agent\n",
       "7050      3  0.115123        agent\n",
       "7050      4  0.127915        agent\n",
       "7050      8  0.063957        agent\n",
       "6206      8  0.995280       agents\n",
       "9704      3  0.998334      airline\n",
       "6007      4  0.995494     airlines\n",
       "993       8  0.993274      airways\n",
       "2434      3  0.990818        allow\n",
       "1554      1  0.991863        amaze\n",
       "2742      2  0.995664     american\n",
       "717       1  0.999832  americanair\n",
       "113       2  0.997453       answer\n",
       "7745      4  0.997539   appreciate\n",
       "6971      5  0.986142         aren\n",
       "510       3  0.974458      arrival\n",
       "6395      4  0.846005       arrive\n",
       "6395      8  0.148205       arrive\n",
       "7295      7  0.995161          ask\n",
       "4623      1  0.981732   assistance\n",
       "1473      5  0.971034      attempt\n",
       "5331      4  0.991290    attendant\n",
       "1263      6  0.983268         auto\n",
       "1041      1  0.980687     automate\n",
       "4962      6  0.992265    available\n",
       "...     ...       ...          ...\n",
       "6459      2  0.994159       update\n",
       "9346      6  0.992340      upgrade\n",
       "6760      1  0.999530    usairways\n",
       "7449      3  0.991280      voucher\n",
       "6094      2  0.998690         wait\n",
       "3683      2  0.450016         want\n",
       "3683      3  0.133865         want\n",
       "3683      4  0.253490         want\n",
       "3683      5  0.125321         want\n",
       "3683      6  0.039875         want\n",
       "7002      8  0.993520         wasn\n",
       "610       2  0.765084      weather\n",
       "610       3  0.232375      weather\n",
       "4291      4  0.989756         week\n",
       "4824      1  0.976504        weeks\n",
       "1778      1  0.987921         wife\n",
       "9172      1  0.608718         work\n",
       "9172      2  0.250193         work\n",
       "9172      5  0.139283         work\n",
       "9336      3  0.992367        world\n",
       "2107      6  0.989327        worry\n",
       "8169      5  0.982206        worse\n",
       "8394      3  0.995758        worst\n",
       "2658      3  0.253339        wrong\n",
       "2658      4  0.740529        wrong\n",
       "3865      8  0.992475         yeah\n",
       "5691      1  0.989833         year\n",
       "4784      1  0.949397    yesterday\n",
       "4784      4  0.038282    yesterday\n",
       "7034      7  0.978843         zero\n",
       "\n",
       "[388 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'ylab': 'PC2', 'xlab': 'PC1'}, topic_order=[5, 2, 8, 1, 4, 7, 3, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
