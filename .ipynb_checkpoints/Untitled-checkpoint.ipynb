{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv('./data/Tweets.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                                                  @VirginAmerica What @dhepburn said.   \n",
       "1                             @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                              @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &...   \n",
       "4                                              @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df = load_data()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (14640, 15)\n",
      "Columns are: Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
      "       'negativereason', 'negativereason_confidence', 'airline',\n",
      "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
      "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
      "       'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      14640 non-null  int64  \n",
      " 1   airline_sentiment             14640 non-null  object \n",
      " 2   airline_sentiment_confidence  14640 non-null  float64\n",
      " 3   negativereason                9178 non-null   object \n",
      " 4   negativereason_confidence     10522 non-null  float64\n",
      " 5   airline                       14640 non-null  object \n",
      " 6   airline_sentiment_gold        40 non-null     object \n",
      " 7   name                          14640 non-null  object \n",
      " 8   negativereason_gold           32 non-null     object \n",
      " 9   retweet_count                 14640 non-null  int64  \n",
      " 10  text                          14640 non-null  object \n",
      " 11  tweet_coord                   1019 non-null   object \n",
      " 12  tweet_created                 14640 non-null  object \n",
      " 13  tweet_location                9907 non-null   object \n",
      " 14  user_timezone                 9820 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size:',tweet_df.shape)\n",
    "print('Columns are:',tweet_df.columns)\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : take \"text\" column (not sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                    @VirginAmerica What @dhepburn said.\n",
       "1                               @VirginAmerica plus you've added commercials to the experience... tacky.\n",
       "2                                @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
       "3    @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &...\n",
       "4                                                @VirginAmerica and it's a really big bad thing about it\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_minus = tweet_df['text']\n",
    "tweet_df_minus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We will perform the following steps: \n",
    "\n",
    "1. **Tokenization** : Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "2. Words that have fewer than 3 characters are removed.\n",
    "3. All **stopwords** are removed.\n",
    "4. Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "5. Words are **stemmed** - words are reduced to their root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zigbo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words_deleted_length = 3\n",
    "Lemmatized = True\n",
    "Stemmed = False\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    if(Lemmatized & Stemmed):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    if(Stemmed):\n",
    "        return stemmer.stem(text)\n",
    "    if(Lemmatized):\n",
    "        return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "    return text\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > Words_deleted_length:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['virginamerica', 'dhepburn', 'say'],\n",
       " ['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = []\n",
    "nan_processed_docs = []\n",
    "for doc in tweet_df_minus :\n",
    "    nan_processed_docs.append(doc)\n",
    "for doc in tweet_df_minus :\n",
    "    processed_docs.append(preprocess(doc))\n",
    "nan_processed_docs[:2]\n",
    "processed_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Bag of words on the dataset\n",
    "\n",
    "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass processed_docs to gensim.corpora.Dictionary() and call it 'dictionary'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dhepburn\n",
      "1 say\n",
      "2 virginamerica\n",
      "3 add\n",
      "4 commercials\n",
      "5 experience\n",
      "6 plus\n",
      "7 tacky\n",
      "8 mean\n",
      "9 need\n",
      "10 today\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(2, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1)]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"virginamerica\") appears 1 time.\n",
      "Word 24 (\"seat\") appears 1 time.\n",
      "Word 28 (\"time\") appears 1 time.\n",
      "Word 88 (\"available\") appears 1 time.\n",
      "Word 89 (\"carriers\") appears 1 time.\n",
      "Word 90 (\"fare\") appears 1 time.\n",
      "Word 91 (\"select\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running LDA using Bag of Words\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "We will be running LDA using all CPU cores to parallelize and speed up model training.\n",
    "\n",
    "Some of the parameters we will be tweaking are:\n",
    "\n",
    "1. **num_topics** is the number of requested latent topics to be extracted from the training corpus.\n",
    "2. **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "3. **workers** is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "4. **alpha** and **beta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is 1/num_topics)\n",
    "\n",
    " **Alpha** is the per document topic distribution.\n",
    "   - *High alpha* : Every document has a mixture of all topics(documents appear similar to each other).\n",
    "   - *Low alpha* : Every document has a mixture of very few topics\n",
    "\n",
    "  **bEta** is the per topic word distribution.\n",
    "    - *High beta* : Each topic has a mixture of most words(topics appear similar to each other).\n",
    "    - *Low eta* : Each topic has a mixture of few words.\n",
    "\n",
    "5. **passes** is the number of training passes through the corpus. For example, if the training corpus has 50,000 documents, chunksize is 10,000, passes is 2, then online training is done in 10 updates:\n",
    "- documents 0-9,999\n",
    "- documents 10,000-19,999\n",
    "- documents 20,000-29,999\n",
    "- documents 30,000-39,999\n",
    "- documents 40,000-49,999\n",
    "- documents 0-9,999\n",
    "- documents 10,000-19,999\n",
    "- documents 20,000-29,999\n",
    "- documents 30,000-39,999\n",
    "- documents 40,000-49,999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 4, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.076*\"americanair\" + 0.039*\"http\" + 0.024*\"jetblue\" + 0.009*\"need\" + 0.007*\"usairways\" + 0.007*\"work\" + 0.006*\"check\" + 0.006*\"southwestair\" + 0.006*\"people\" + 0.005*\"good\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.044*\"jetblue\" + 0.044*\"flight\" + 0.037*\"unite\" + 0.025*\"delay\" + 0.019*\"usairways\" + 0.017*\"southwestair\" + 0.016*\"plane\" + 0.013*\"thank\" + 0.013*\"americanair\" + 0.011*\"http\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.056*\"americanair\" + 0.045*\"usairways\" + 0.033*\"service\" + 0.031*\"thank\" + 0.025*\"customer\" + 0.024*\"unite\" + 0.014*\"southwestair\" + 0.009*\"wait\" + 0.009*\"time\" + 0.009*\"luggage\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.087*\"flight\" + 0.059*\"americanair\" + 0.033*\"cancel\" + 0.023*\"usairways\" + 0.019*\"southwestair\" + 0.017*\"help\" + 0.016*\"flightled\" + 0.015*\"hold\" + 0.014*\"book\" + 0.013*\"try\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: create T_e_i an input to have $t_{ei}$ after apply Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['americanair',\n",
       "  'http',\n",
       "  'jetblue',\n",
       "  'need',\n",
       "  'usairways',\n",
       "  'work',\n",
       "  'check',\n",
       "  'southwestair',\n",
       "  'people',\n",
       "  'good'],\n",
       " ['jetblue',\n",
       "  'flight',\n",
       "  'unite',\n",
       "  'delay',\n",
       "  'usairways',\n",
       "  'southwestair',\n",
       "  'plane',\n",
       "  'thank',\n",
       "  'americanair',\n",
       "  'http'],\n",
       " ['americanair',\n",
       "  'usairways',\n",
       "  'service',\n",
       "  'thank',\n",
       "  'customer',\n",
       "  'unite',\n",
       "  'southwestair',\n",
       "  'wait',\n",
       "  'time',\n",
       "  'luggage'],\n",
       " ['flight',\n",
       "  'americanair',\n",
       "  'cancel',\n",
       "  'usairways',\n",
       "  'southwestair',\n",
       "  'help',\n",
       "  'flightled',\n",
       "  'hold',\n",
       "  'book',\n",
       "  'try']]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_topics=lda_model.show_topics(formatted=False)\n",
    "list_topics \n",
    "\n",
    "topic = []\n",
    "T_e_i = []\n",
    "for tup in list_topics:\n",
    "    topic = []\n",
    "    for tup2 in tup[1]:\n",
    "        topic.append(tup2[0])\n",
    "    T_e_i.append(topic) \n",
    "    \n",
    "T_e_i  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Train Doc2Vec on non-pre-processed data \"nan_processed_docs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "def read(fname):\n",
    "    for i, line in enumerate(f):\n",
    "        tokens = fname[i]\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_processed_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(nan_processed_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_nan_processed_docs)\n",
    "model.train(train_nan_processed_docs, total_examples=len(nan_processed_docs), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Apply Doc2Vec on each document in your data to form $d_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['virginamerica', 'dhepburn', 'say'], ['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']]\n",
      "[array([-8.1121130e-03, -8.2422188e-03, -8.5009653e-03,  9.4700670e-03,\n",
      "       -6.1424044e-03, -6.0291924e-03,  9.3967384e-03, -3.2938537e-03,\n",
      "       -7.4124401e-03,  4.4365735e-03,  3.3982443e-03,  5.7742302e-03,\n",
      "       -2.0059585e-03, -6.6704517e-03,  3.3299492e-03,  8.7660858e-03,\n",
      "        3.0309090e-03,  9.5953336e-03,  2.2019406e-03, -3.9082011e-03,\n",
      "       -5.5418705e-04, -7.8981556e-03,  1.1231005e-03, -7.9345554e-03,\n",
      "        2.9577192e-05,  9.9902209e-03,  5.6541548e-03, -1.1336298e-03,\n",
      "       -9.5684947e-03,  7.5632879e-03,  3.8973801e-03,  1.7543974e-04,\n",
      "        3.7038026e-03,  6.9300961e-03, -5.4608597e-03,  5.8965799e-03,\n",
      "       -8.2795974e-03, -2.2272721e-03,  1.6476009e-03,  4.3660291e-03,\n",
      "        3.5117818e-03,  3.2109192e-03, -2.7814955e-03, -6.1880853e-03,\n",
      "        8.8355839e-03, -2.6708855e-03,  3.3353553e-03,  7.7089434e-03,\n",
      "        3.2863969e-03, -6.2256311e-03], dtype=float32), array([-0.00936932, -0.00698564, -0.00864459, -0.00804482, -0.00636543,\n",
      "       -0.00064517, -0.00104701, -0.00397581,  0.00162256,  0.00684583,\n",
      "        0.00209305, -0.00765901, -0.00698227,  0.00916744, -0.00988943,\n",
      "        0.00389979,  0.00556994,  0.00624627, -0.00465707,  0.00135264,\n",
      "        0.0082452 , -0.00157203, -0.00198241,  0.00060964,  0.00593629,\n",
      "        0.00490234, -0.00305704,  0.0047025 ,  0.00374744,  0.00643385,\n",
      "        0.00238679,  0.00585797, -0.00134428, -0.00551607,  0.00303706,\n",
      "        0.00999977,  0.00207013, -0.00232197, -0.00882894, -0.00261495,\n",
      "        0.00425575, -0.00238884,  0.00022205, -0.00496993,  0.00836674,\n",
      "        0.00710091, -0.00202338, -0.006395  ,  0.00733856,  0.00524756],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "Liste_D_e_i = []\n",
    "Liste_of_n_docs = processed_docs\n",
    "for i in range(len(Liste_of_n_docs)):\n",
    "    vector = []\n",
    "    vector = model.infer_vector(Liste_of_n_docs[i])\n",
    "    Liste_D_e_i.append(vector) \n",
    "    \n",
    "print(Liste_of_n_docs[:2])\n",
    "print(Liste_D_e_i[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Apply Doc2Vec on your topics to form $t_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.6189611e-03, -5.7761609e-03,  8.8887429e-03,  7.1812181e-05,\n",
       "         1.3372501e-03,  2.2232926e-03,  3.3773489e-03, -5.0686724e-03,\n",
       "         2.3342411e-03,  9.5074857e-03, -5.6510912e-03,  4.9197194e-03,\n",
       "        -6.3650357e-03,  8.8025462e-03, -4.6121841e-03, -4.7187209e-03,\n",
       "        -6.0499255e-03,  8.9482125e-03,  4.5692851e-03, -1.5405561e-04,\n",
       "        -4.2776135e-03,  7.6732994e-03, -9.7185532e-03, -1.5098313e-03,\n",
       "         2.4043793e-04,  3.5099888e-03,  6.6660470e-03, -6.9918861e-03,\n",
       "         7.2210683e-03, -9.8983021e-03,  1.9597057e-03, -1.4058575e-03,\n",
       "         4.4692433e-03, -7.0016026e-03,  4.2176726e-03, -7.0681684e-03,\n",
       "         9.9102091e-03, -7.9297964e-03,  6.6172373e-03,  9.6123023e-03,\n",
       "         5.0199782e-03,  8.7301964e-03, -4.1522384e-03,  4.8934389e-03,\n",
       "         2.2044850e-03,  9.0630125e-04,  3.2948253e-03, -8.0621773e-03,\n",
       "         4.5056785e-03,  5.5571552e-03], dtype=float32),\n",
       " array([ 0.00275685, -0.00764542, -0.009705  ,  0.00741494, -0.00103935,\n",
       "         0.00027869, -0.0067296 ,  0.00251352,  0.0074662 ,  0.0029708 ,\n",
       "        -0.00459028, -0.00665765,  0.00294651,  0.0040573 , -0.00127593,\n",
       "         0.0034583 ,  0.00215372,  0.00405285, -0.00085828,  0.00428053,\n",
       "        -0.00556354, -0.00685003, -0.00590697,  0.0018979 , -0.0094387 ,\n",
       "         0.00668117,  0.00322717,  0.00258185,  0.00610396,  0.0065759 ,\n",
       "         0.00245959, -0.0097196 ,  0.00572324,  0.00972706, -0.00447755,\n",
       "        -0.00908937,  0.00093324, -0.0015826 ,  0.00056194, -0.0033917 ,\n",
       "        -0.00591095, -0.00576014, -0.0037935 ,  0.0016104 , -0.00111417,\n",
       "        -0.00323938,  0.00970042,  0.00949954, -0.00420288,  0.00105828],\n",
       "       dtype=float32),\n",
       " array([ 0.00599192,  0.00100532,  0.00875409,  0.00976201,  0.00935974,\n",
       "         0.00202746,  0.00855513, -0.00647031, -0.00064462,  0.00887595,\n",
       "        -0.00468342, -0.00302101,  0.00836356, -0.00967531, -0.00430546,\n",
       "         0.00183403, -0.00857729, -0.00306362,  0.00334355, -0.00780206,\n",
       "        -0.00086573, -0.00140614,  0.00260861, -0.00373118, -0.00185513,\n",
       "         0.00753097,  0.00364708, -0.00500581, -0.00697556,  0.00407365,\n",
       "        -0.00215342,  0.00349132,  0.00824069,  0.00794226, -0.0098096 ,\n",
       "         0.00553361, -0.007096  ,  0.00521463, -0.00391978,  0.00564434,\n",
       "         0.00492334,  0.00775931, -0.00859189,  0.00957658,  0.00393944,\n",
       "         0.00421635, -0.00698891, -0.00261936,  0.00710116,  0.0084095 ],\n",
       "       dtype=float32),\n",
       " array([-0.00518229,  0.00577825, -0.0070497 , -0.00297502,  0.00023288,\n",
       "        -0.00078597, -0.00405514, -0.00231595,  0.00048441,  0.00350722,\n",
       "         0.00818617, -0.00966074, -0.00046124,  0.00940925,  0.00475797,\n",
       "         0.00661796, -0.00754994, -0.00399259,  0.00893895,  0.0048753 ,\n",
       "         0.00994074, -0.00684551,  0.00258625, -0.00343048,  0.00603482,\n",
       "        -0.00479578, -0.00191584, -0.00451509, -0.00246861,  0.00247391,\n",
       "        -0.00012477,  0.00128608,  0.00422127, -0.00330136,  0.00260175,\n",
       "         0.0006889 ,  0.00913358, -0.00607071,  0.00427584, -0.00535246,\n",
       "         0.00462302, -0.00774266, -0.0013601 ,  0.00387053,  0.00355186,\n",
       "         0.00185673,  0.00341001, -0.00866817,  0.0076493 , -0.00151843],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Liste_T_e_i = []\n",
    "for i in range(len(T_e_i)):\n",
    "    vector = []\n",
    "    vector = model.infer_vector(T_e_i[i])\n",
    "    Liste_T_e_i.append(vector) \n",
    "Liste_T_e_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Calculate the similarity between every pair $d_{ei}$ and $t_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
