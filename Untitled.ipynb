{
 "cells": [
  {
   "source": [
    "## Text project"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv('./data/Tweets.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                                                  @VirginAmerica What @dhepburn said.   \n",
       "1                             @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                              @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &...   \n",
       "4                                              @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>cairdin</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>NaN</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>neutral</td>\n      <td>0.6837</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>yvonnalynn</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:48 -0800</td>\n      <td>Lets Play</td>\n      <td>Central Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing about it</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tweet_df = load_data()\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset size: (14640, 15)\nColumns are: Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n       'negativereason', 'negativereason_confidence', 'airline',\n       'airline_sentiment_gold', 'name', 'negativereason_gold',\n       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n       'tweet_location', 'user_timezone'],\n      dtype='object')\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 14640 entries, 0 to 14639\nData columns (total 15 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   tweet_id                      14640 non-null  int64  \n 1   airline_sentiment             14640 non-null  object \n 2   airline_sentiment_confidence  14640 non-null  float64\n 3   negativereason                9178 non-null   object \n 4   negativereason_confidence     10522 non-null  float64\n 5   airline                       14640 non-null  object \n 6   airline_sentiment_gold        40 non-null     object \n 7   name                          14640 non-null  object \n 8   negativereason_gold           32 non-null     object \n 9   retweet_count                 14640 non-null  int64  \n 10  text                          14640 non-null  object \n 11  tweet_coord                   1019 non-null   object \n 12  tweet_created                 14640 non-null  object \n 13  tweet_location                9907 non-null   object \n 14  user_timezone                 9820 non-null   object \ndtypes: float64(2), int64(2), object(11)\nmemory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size:',tweet_df.shape)\n",
    "print('Columns are:',tweet_df.columns)\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : take \"text\" column (not sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                                                                    @VirginAmerica What @dhepburn said.\n",
       "1                               @VirginAmerica plus you've added commercials to the experience... tacky.\n",
       "2                                @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
       "3    @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &...\n",
       "4                                                @VirginAmerica and it's a really big bad thing about it\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tweet_df_minus = tweet_df['text']\n",
    "tweet_df_minus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Data Preprocessing\n",
    "\n",
    "We will perform the following steps: \n",
    "\n",
    "1. **Tokenization** : Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "2. Words that have fewer than 3 characters are removed.\n",
    "3. All **stopwords** are removed.\n",
    "4. Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "5. Words are **stemmed** - words are reduced to their root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\zigbo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words_deleted_length = 3\n",
    "Lemmatized = True\n",
    "Stemmed = False\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    if(Lemmatized & Stemmed):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    if(Stemmed):\n",
    "        return stemmer.stem(text)\n",
    "    if(Lemmatized):\n",
    "        return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "    return text\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > Words_deleted_length:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['virginamerica', 'dhepburn', 'say'],\n",
       " ['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "processed_docs = []\n",
    "nan_processed_docs = []\n",
    "for doc in tweet_df_minus :\n",
    "    nan_processed_docs.append(doc)\n",
    "for doc in tweet_df_minus :\n",
    "    processed_docs.append(preprocess(doc))\n",
    "nan_processed_docs[:2]\n",
    "processed_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Bag of words on the dataset\n",
    "\n",
    "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass processed_docs to gensim.corpora.Dictionary() and call it 'dictionary'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 dhepburn\n1 say\n2 virginamerica\n3 add\n4 commercials\n5 experience\n6 plus\n7 tacky\n8 mean\n9 need\n10 today\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nOPTIONAL STEP\\nRemove very rare and very common words:\\n\\n- words appearing less than 15 times\\n- words appearing in more than 10% of all documents\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(2, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word 2 (\"virginamerica\") appears 1 time.\nWord 24 (\"seat\") appears 1 time.\nWord 28 (\"time\") appears 1 time.\nWord 88 (\"available\") appears 1 time.\nWord 89 (\"carriers\") appears 1 time.\nWord 90 (\"fare\") appears 1 time.\nWord 91 (\"select\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Running LDA using Bag of Words\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "We will be running LDA using all CPU cores to parallelize and speed up model training.\n",
    "\n",
    "Some of the parameters we will be tweaking are:\n",
    "\n",
    "1. **num_topics** is the number of requested latent topics to be extracted from the training corpus.\n",
    "2. **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "3. **workers** is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "4. **alpha** and **beta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is 1/num_topics)\n",
    "\n",
    " **Alpha** is the per document topic distribution.\n",
    "   - *High alpha* : Every document has a mixture of all topics(documents appear similar to each other).\n",
    "   - *Low alpha* : Every document has a mixture of very few topics\n",
    "\n",
    "  **bEta** is the per topic word distribution.\n",
    "    - *High beta* : Each topic has a mixture of most words(topics appear similar to each other).\n",
    "    - *Low eta* : Each topic has a mixture of few words.\n",
    "\n",
    "5. **passes** is the number of training passes through the corpus. For example, if the training corpus has 50,000 documents, chunksize is 10,000, passes is 2, then online training is done in 10 updates:\n",
    "- documents 0-9,999\n",
    "- documents 10,000-19,999\n",
    "- documents 20,000-29,999\n",
    "- documents 30,000-39,999\n",
    "- documents 40,000-49,999\n",
    "- documents 0-9,999\n",
    "- documents 10,000-19,999\n",
    "- documents 20,000-29,999\n",
    "- documents 30,000-39,999\n",
    "- documents 40,000-49,999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 4, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \nWords: 0.053*\"jetblue\" + 0.042*\"unite\" + 0.036*\"thank\" + 0.026*\"americanair\" + 0.014*\"usairways\" + 0.011*\"http\" + 0.010*\"like\" + 0.010*\"flight\" + 0.009*\"send\" + 0.007*\"follow\"\n\n\nTopic: 1 \nWords: 0.085*\"southwestair\" + 0.034*\"jetblue\" + 0.025*\"http\" + 0.021*\"usairways\" + 0.019*\"flight\" + 0.014*\"delay\" + 0.009*\"fly\" + 0.009*\"thank\" + 0.007*\"love\" + 0.007*\"plane\"\n\n\nTopic: 2 \nWords: 0.055*\"americanair\" + 0.029*\"usairways\" + 0.029*\"flight\" + 0.027*\"unite\" + 0.020*\"wait\" + 0.018*\"time\" + 0.012*\"hour\" + 0.012*\"hold\" + 0.010*\"gate\" + 0.010*\"hours\"\n\n\nTopic: 3 \nWords: 0.084*\"flight\" + 0.076*\"americanair\" + 0.032*\"usairways\" + 0.032*\"cancel\" + 0.017*\"service\" + 0.015*\"help\" + 0.015*\"flightled\" + 0.014*\"customer\" + 0.010*\"try\" + 0.009*\"tomorrow\"\n\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: create T_e_i an input to have $t_{ei}$ after apply Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['jetblue',\n",
       "  'unite',\n",
       "  'thank',\n",
       "  'americanair',\n",
       "  'usairways',\n",
       "  'http',\n",
       "  'like',\n",
       "  'flight',\n",
       "  'send',\n",
       "  'follow'],\n",
       " ['southwestair',\n",
       "  'jetblue',\n",
       "  'http',\n",
       "  'usairways',\n",
       "  'flight',\n",
       "  'delay',\n",
       "  'fly',\n",
       "  'thank',\n",
       "  'love',\n",
       "  'plane'],\n",
       " ['americanair',\n",
       "  'usairways',\n",
       "  'flight',\n",
       "  'unite',\n",
       "  'wait',\n",
       "  'time',\n",
       "  'hour',\n",
       "  'hold',\n",
       "  'gate',\n",
       "  'hours'],\n",
       " ['flight',\n",
       "  'americanair',\n",
       "  'usairways',\n",
       "  'cancel',\n",
       "  'service',\n",
       "  'help',\n",
       "  'flightled',\n",
       "  'customer',\n",
       "  'try',\n",
       "  'tomorrow']]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "list_topics=lda_model.show_topics(formatted=False)\n",
    "list_topics \n",
    "\n",
    "topic = []\n",
    "T_e_i = []\n",
    "for tup in list_topics:\n",
    "    topic = []\n",
    "    for tup2 in tup[1]:\n",
    "        topic.append(tup2[0])\n",
    "    T_e_i.append(topic) \n",
    "    \n",
    "T_e_i  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Train Doc2Vec on non-pre-processed data \"nan_processed_docs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "def read(fname):\n",
    "    for i, line in enumerate(f):\n",
    "        tokens = fname[i]\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_processed_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(nan_processed_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e8191444b7f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_nan_processed_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_nan_processed_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnan_processed_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_nan_processed_docs)\n",
    "model.train(train_nan_processed_docs, total_examples=len(nan_processed_docs), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Apply Doc2Vec on each document in your data to form $d_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['virginamerica', 'dhepburn', 'say'], ['virginamerica', 'plus', 'add', 'commercials', 'experience', 'tacky']]\n[array([ 1.41634396e-03,  4.92199510e-03,  5.72341355e-03,  4.96392697e-03,\n       -9.20800085e-05,  9.69085004e-03, -2.69359001e-03, -5.74699463e-03,\n       -1.16604562e-04, -8.96097254e-03, -6.90191798e-03, -1.80953197e-04,\n        3.01526301e-03, -3.25171393e-03, -1.51507009e-03, -2.65509589e-03,\n       -1.02217396e-04, -9.32262000e-03,  4.27722419e-03, -9.91559494e-03,\n        4.40249307e-04,  5.65011194e-03,  6.59355894e-03, -4.17658797e-04,\n       -2.71536759e-04, -5.54219354e-03, -5.31006837e-04, -9.61865298e-03,\n        9.79232881e-03,  4.53716191e-03, -3.27477767e-03, -2.20434717e-03,\n        1.11968206e-04,  5.58489421e-03, -6.54767966e-03, -5.08527039e-03,\n       -7.48659763e-03, -6.47957670e-03,  5.07089682e-03,  9.36201215e-03,\n       -8.48779629e-04,  3.73431505e-03,  3.47155286e-03, -2.88870931e-03,\n        1.18789868e-03, -7.73159135e-03, -8.08762386e-03, -8.57015979e-03,\n       -4.78750002e-03, -4.70462209e-03], dtype=float32), array([-7.4141063e-03, -4.9680485e-03, -7.6080714e-03, -1.8396393e-03,\n        7.1734996e-03,  2.3223562e-03,  2.7810640e-03,  5.5311229e-03,\n       -5.0648679e-03, -5.7731438e-03,  8.6603472e-03, -9.8657943e-03,\n       -1.6468646e-03, -7.9418840e-03, -2.6270372e-04,  8.3503025e-03,\n        5.6809243e-03, -9.6844276e-03,  5.8223479e-05,  3.1154326e-03,\n       -3.7744273e-03,  8.5811950e-03,  2.1658246e-03, -1.0883940e-03,\n        5.1727160e-03,  7.7410480e-03,  3.1424339e-03, -2.4709625e-03,\n        1.2867535e-03,  2.3860773e-03,  4.7953639e-04,  7.4450220e-03,\n       -8.0845300e-03, -3.0778337e-03, -9.0940492e-03, -5.7914788e-03,\n        3.9227568e-03, -4.8409333e-03,  8.0281548e-04,  3.5173537e-03,\n       -2.9272779e-03, -8.7172389e-03,  9.1468170e-03,  8.5338857e-04,\n       -6.8197544e-03,  7.3133879e-03,  5.3365314e-03, -8.3323531e-03,\n        2.5708913e-03,  2.3268722e-03], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "Liste_D_e_i = []\n",
    "Liste_of_n_docs = processed_docs\n",
    "for i in range(len(Liste_of_n_docs)):\n",
    "    vector = []\n",
    "    vector = model.infer_vector(Liste_of_n_docs[i])\n",
    "    Liste_D_e_i.append(vector) \n",
    "    \n",
    "print(Liste_of_n_docs[:2])\n",
    "print(Liste_D_e_i[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Apply Doc2Vec on your topics to form $t_{ei}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([ 0.00377258, -0.00587244,  0.0062098 , -0.00365238,  0.00888249,\n",
       "        -0.00613669, -0.00593472, -0.00708308, -0.00149424, -0.00233099,\n",
       "        -0.00924337, -0.00587347,  0.00556565, -0.00384947, -0.00400359,\n",
       "        -0.00797708,  0.00999407,  0.00515376,  0.00989509, -0.00692765,\n",
       "        -0.00421472,  0.00757083, -0.00480382, -0.00619613,  0.00159725,\n",
       "        -0.00051465, -0.0074537 , -0.00137075, -0.00988115,  0.00514072,\n",
       "         0.00090565, -0.00055936, -0.00450426,  0.00705291,  0.00612202,\n",
       "        -0.0063826 ,  0.00011322, -0.00409659, -0.00041246, -0.0007189 ,\n",
       "        -0.00552521, -0.0019561 ,  0.00255773,  0.00742982,  0.00580838,\n",
       "        -0.00112513,  0.00806797,  0.00335233, -0.00399448, -0.00178039],\n",
       "       dtype=float32),\n",
       " array([ 0.0075329 ,  0.00176699,  0.00077735, -0.00991786,  0.00055435,\n",
       "        -0.00462698,  0.00490848,  0.00237821,  0.00083664,  0.00424549,\n",
       "         0.00793436, -0.00161116, -0.00658085, -0.00987477, -0.00081734,\n",
       "        -0.00034996,  0.0016382 , -0.00123536,  0.00237909, -0.00610803,\n",
       "        -0.00301529, -0.00533313,  0.00077411,  0.00619016, -0.00405402,\n",
       "        -0.00935377,  0.00170854, -0.00744018, -0.00088563,  0.00311737,\n",
       "         0.00745565,  0.00927283, -0.0067824 ,  0.0085887 , -0.0069011 ,\n",
       "        -0.00164177, -0.0075833 ,  0.00856032, -0.00526337,  0.00071552,\n",
       "         0.00597235,  0.00733476,  0.00370077, -0.00510452, -0.00748251,\n",
       "        -0.00954865,  0.00520815,  0.00126681,  0.00688675, -0.00732972],\n",
       "       dtype=float32),\n",
       " array([ 0.00296661,  0.00154931,  0.00069243, -0.00369751, -0.00496407,\n",
       "         0.00975813, -0.00407588, -0.00780631, -0.00499659, -0.00315588,\n",
       "         0.0019643 , -0.00539176,  0.00086975,  0.00532236,  0.00068331,\n",
       "         0.00876686,  0.00625938, -0.00246062,  0.00258398, -0.00295742,\n",
       "        -0.00530989,  0.00661255, -0.00742196, -0.00453236,  0.00220478,\n",
       "         0.00269359,  0.00515311,  0.00717628,  0.00082016,  0.00886877,\n",
       "         0.0093752 ,  0.00932299, -0.00699949,  0.00877145, -0.00392667,\n",
       "         0.00019169,  0.00764803, -0.00062754, -0.00914057, -0.00922303,\n",
       "        -0.00115461, -0.00196487,  0.00308568,  0.0028456 ,  0.00491725,\n",
       "         0.00013468, -0.00066924, -0.00858849, -0.00644128,  0.00962245],\n",
       "       dtype=float32),\n",
       " array([-0.00221477, -0.00669979, -0.00321184,  0.00700899, -0.00465915,\n",
       "        -0.00331719,  0.00329413, -0.00896292, -0.00888682,  0.00223017,\n",
       "         0.00916622, -0.00287843,  0.00054532, -0.00228458,  0.00365093,\n",
       "         0.00200588, -0.00404468,  0.00174212,  0.00526777, -0.00667528,\n",
       "         0.00624045,  0.00778569, -0.00312064, -0.00954066,  0.00091515,\n",
       "        -0.00416045,  0.00723011,  0.00311394, -0.00047862, -0.00814055,\n",
       "        -0.00775586, -0.00054032, -0.00819979,  0.00548919, -0.009707  ,\n",
       "        -0.00477754, -0.00041158, -0.00104336, -0.00157404,  0.006074  ,\n",
       "        -0.00161352, -0.00233955,  0.00945774, -0.00670365,  0.00453061,\n",
       "        -0.00683093,  0.00561608,  0.00176046, -0.00127001,  0.0021812 ],\n",
       "       dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "Liste_T_e_i = []\n",
    "for i in range(len(T_e_i)):\n",
    "    vector = []\n",
    "    vector = model.infer_vector(T_e_i[i])\n",
    "    Liste_T_e_i.append(vector) \n",
    "Liste_T_e_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Calculate the similarity between every pair $d_{ei}$ and $t_{ei}$\n",
    "    Dont run this code bcz Liste_D_e_i length != Liste_T_e_i length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-6210b9398d42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msimilairty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mListe_D_e_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msimilairty\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mListe_D_e_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mListe_T_e_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "similairty = []\n",
    "for i in Liste_D_e_i:\n",
    "    similairty[i] = spatial.distance.cosine(Liste_D_e_i[i], Liste_T_e_i[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}